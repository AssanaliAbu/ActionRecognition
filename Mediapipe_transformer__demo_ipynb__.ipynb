{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SomPcIvmaI7Z"
      },
      "source": [
        "# Transformer model on KTH dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the project path to the appropriate project folder path"
      ],
      "metadata": {
        "id": "pIUKSDpbqmj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install tensorflow==2.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2T2RycpCoir",
        "outputId": "47d52602-e103-45be-8e13-387a4330aa0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.7\n",
            "  Downloading tensorflow-2.7.0-cp39-cp39-manylinux2010_x86_64.whl (489.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.7/489.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7) (1.16.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7) (0.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7) (1.6.3)\n",
            "Collecting flatbuffers<3.0,>=1.12\n",
            "  Downloading flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
            "Collecting keras<2.8,>=2.7.0rc0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7) (2.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7) (1.53.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7) (4.5.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7) (1.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7) (3.20.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7) (0.32.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7) (1.22.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7) (0.40.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7) (16.0.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7) (3.8.0)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.1/463.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.1.1\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7) (2.12.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7) (3.3.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (0.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (2.27.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (1.0.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (2.2.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (67.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (2.17.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard~=2.6->tensorflow==2.7) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7) (6.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.6->tensorflow==2.7) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard~=2.6->tensorflow==2.7) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, flatbuffers, keras-preprocessing, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 23.3.3\n",
            "    Uninstalling flatbuffers-23.3.3:\n",
            "      Successfully uninstalled flatbuffers-23.3.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed flatbuffers-2.0.7 keras-2.7.0 keras-preprocessing-1.1.2 tensorflow-2.7.0 tensorflow-estimator-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path= 'drive/MyDrive/HumanActionRecognition/'"
      ],
      "metadata": {
        "id": "mAxaQBxHbPNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fudwk14WbPbB",
        "outputId": "96c0355b-66e7-482b-fe08-ba0ce1147c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/HumanActionRecognition/Utils/tools/an_example_skeleton_of_standing.py ."
      ],
      "metadata": {
        "id": "DTMpHTE2eiOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/HumanActionRecognition/Utils/utils/lib_plot.py ."
      ],
      "metadata": {
        "id": "f7W-7CkSeiWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/HumanActionRecognition/Utils/utils/lib_commons.py ."
      ],
      "metadata": {
        "id": "yJp9lAfCeiZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simplejson"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lay3JRUmeieB",
        "outputId": "20b40eba-c3a3-4af3-8f53-09e9dcea0e92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting simplejson\n",
            "  Downloading simplejson-3.19.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: simplejson\n",
            "Successfully installed simplejson-3.19.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwprds_-aI7p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "import random\n",
        "from random import randint\n",
        "import time\n",
        "import os\n",
        "from scipy.spatial.distance import pdist\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd\n",
        "import imageio\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hDDGHQiaI7s"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import simplejson\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from an_example_skeleton_of_standing import get_a_normalized_jhmdb_skeleton\n",
        "from collections import deque\n",
        "import pickle\n",
        "import sklearn.model_selection\n",
        "from sklearn.metrics import classification_report\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import PCA\n",
        "import lib_plot as lib_plot\n",
        "import lib_commons as lib_commons\n",
        "import math\n",
        "from keras.regularizers import l2\n",
        "from tensorflow.keras.layers import BatchNormalization\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def draw(skeleton): # helper function to draw skeleton\n",
        "    x = skeleton[0::2]\n",
        "    y = skeleton[1::2]\n",
        "    plt.plot(x, y, \"r*\")\n",
        "    plt.axis(\"equal\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "GRovC4Jgh0Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD_CgomQaI7w"
      },
      "source": [
        "\n",
        "#Data augmentation:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Flips original data along x-axis\n",
        "2. Adds Gaussian noise to original data with sigma 0.02\n",
        "3. Adds Gaussian noise to original data with sigma 0.04\n",
        "4. Rotates flipped data to 15°\n",
        "5. Rotates flipped data to -15°\n",
        "6. Rotates flipped data to 30°\n",
        "7. Rotates flipped data to -30°"
      ],
      "metadata": {
        "id": "ZoxuapC5P7bP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_augment(x_df, y_df, size):\n",
        "    #flipping\n",
        "    x_copy = x_df.copy()\n",
        "    x_copy[x_copy.columns[::2]] = x_copy[x_copy.columns[::2]].apply(lambda x: -x)\n",
        "    x_flipped = pd.concat([x_df, x_copy], ignore_index=True)\n",
        "    y_copy = y_df.copy()\n",
        "    y_flipped = pd.concat([y_df, y_copy], ignore_index=True)\n",
        "    #noising\n",
        "    mu, sigma1 = 0, 0.02\n",
        "    noise = np.random.normal(mu, sigma1, [size,30])\n",
        "    x_noise = x_df + noise\n",
        "    x_noised = pd.concat([x_flipped, x_noise], ignore_index=True)\n",
        "    y_noised = pd.concat([y_flipped, y_copy], ignore_index=True)\n",
        "    #noising2\n",
        "    sigma2 = 0.04\n",
        "    noise2 = np.random.normal(mu, sigma2, [size,30])\n",
        "    x_noise2 = x_df + noise2\n",
        "    x_noised2 = pd.concat([x_noised, x_noise2], ignore_index=True)\n",
        "    y_noised2 = pd.concat([y_noised, y_copy], ignore_index=True)\n",
        "    # rotate1\n",
        "    x_rotate1 = x_flipped.apply(lambda x: rotate(x.to_numpy(), 15), axis=1, result_type='broadcast')\n",
        "    x_rotated = pd.concat([x_noised2, x_rotate1], ignore_index=True)\n",
        "    y_rotated = pd.concat([y_noised2, y_flipped], ignore_index=True)\n",
        "    #rotate2\n",
        "    x_rotate2 = x_flipped.apply(lambda x: rotate(x.to_numpy(), -15), axis=1, result_type='broadcast')\n",
        "    x_rotated2 = pd.concat([x_rotated, x_rotate2], ignore_index=True)\n",
        "    y_rotated2 = pd.concat([y_rotated, y_flipped], ignore_index=True)\n",
        "    #rotate3\n",
        "    x_rotate3 = x_flipped.apply(lambda x: rotate(x.to_numpy(), 30), axis=1, result_type='broadcast')\n",
        "    x_rotated3 = pd.concat([x_rotated2, x_rotate3], ignore_index=True)\n",
        "    y_rotated3 = pd.concat([y_rotated2, y_flipped], ignore_index=True)\n",
        "    # rotate4\n",
        "    x_rotate4 = x_flipped.apply(lambda x: rotate(x.to_numpy(), -30), axis=1, result_type='broadcast')\n",
        "    x_rotated4 = pd.concat([x_rotated3, x_rotate4], ignore_index=True)\n",
        "    y_rotated4 = pd.concat([y_rotated3, y_flipped], ignore_index=True)\n",
        "\n",
        "    return x_rotated4, y_rotated4"
      ],
      "metadata": {
        "id": "6XPVUWtah0U2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_joint(joint: tuple, p: int) -> tuple:\n",
        "    x, y = joint\n",
        "    psi = np.deg2rad(p)\n",
        "\n",
        "    a = np.array([[np.cos(psi), -np.sin(psi)],\n",
        "                  [np.sin(psi), np.cos(psi)]])\n",
        "    b = np.array([x, y])\n",
        "    joint = np.matmul(a, b)\n",
        "    return joint\n",
        "\n",
        "def rotate(frame, p: int) -> np.array:\n",
        "    frame = frame.reshape(-1, 2)\n",
        "    rotated = np.array([rotate_joint(joint=joint, p=p) for joint in frame])\n",
        "    return rotated.reshape(-1)"
      ],
      "metadata": {
        "id": "vuA4PdGNAOmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZszNSH23aI8B"
      },
      "source": [
        "# Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEkC2togaI7w"
      },
      "outputs": [],
      "source": [
        "WINDOW_SIZE = int(5) # number of frames in action-snippet used to extract features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENOSWP0HaI7w"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "CLASSES = np.array(['0', '1', '2', '3', '4', '5'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"/content/drive/MyDrive/HRNET/Train_cut\", \"rb\") as fp:   # Unpickling\n",
        "#   train = pickle.load(fp)\n",
        "\n",
        "# with open(\"/content/drive/MyDrive/HRNET/Val_cut\", \"rb\") as fp:   # Unpickling\n",
        "#   val = pickle.load(fp)\n",
        "\n",
        "# with open(\"/content/drive/MyDrive/HRNET/Test_cut\", \"rb\") as fp:   # Unpickling\n",
        "#   test = pickle.load(fp)\n",
        "\n",
        "train = np.load('/content/drive/MyDrive/Abu_KTH/Mediapipe/train_poses.npy', allow_pickle=True)\n",
        "val = np.load('/content/drive/MyDrive/Abu_KTH/Mediapipe/val_poses.npy', allow_pickle=True)\n",
        "test = np.load('/content/drive/MyDrive/Abu_KTH/Mediapipe/test_poses.npy', allow_pickle=True)\n",
        "train = train.tolist()\n",
        "val = val.tolist()\n",
        "test = test.tolist()"
      ],
      "metadata": {
        "id": "HQD574BaDi2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train + val\n",
        "train_labels = train_labels + val_labels"
      ],
      "metadata": {
        "id": "26Kl66n9H6j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zoom(p, target_l=32, joints_num=33, joints_dim=2):\n",
        "    l = p.shape[0]\n",
        "    p_new = np.empty([target_l, joints_num, joints_dim])\n",
        "    for m in range(joints_num):\n",
        "        for n in range(joints_dim):\n",
        "            p[:, m, n] = medfilt(p[:, m, n], 3)\n",
        "            p_new[:, m, n] = inter.zoom(p[:, m, n], target_l / l)[:target_l]\n",
        "    return p_new\n"
      ],
      "metadata": {
        "id": "hLyN4TPfSEkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.random.rand(300, 33, 2)\n",
        "\n",
        "# Access every tenth row\n",
        "arr_every_tenth_row = arr[::10]\n",
        "\n",
        "# Print the shape of the resulting array\n",
        "print(arr_every_tenth_row.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ7ZxWMrG_Ec",
        "outputId": "b91859ef-96a2-4815-9c74-e8fe6ac7dd81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30, 33, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.signal import medfilt\n",
        "import scipy.ndimage.interpolation as inter\n",
        "# Example data\n",
        "\n",
        "# Example data\n",
        "data = []\n",
        "for video in train:\n",
        "  video  = video[::10]\n",
        "  video = zoom(video)\n",
        "  data.append(video)\n",
        "\n",
        "number_Of_videos = len(data)\n",
        "\n",
        "# Reshape and concatenate the data for each frame of each video\n",
        "frames = []\n",
        "for i in range(number_Of_videos):\n",
        "    for j in range(0, data[i].shape[0]):\n",
        "        print(data[i].shape)\n",
        "        reshaped_data = data[i][j].reshape((1, 33*2))\n",
        "        frames.append(pd.DataFrame(data=reshaped_data))\n",
        "print(len(frames))\n",
        "# Concatenate the data frames into a single DataFrame\n",
        "df_train = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "# Create column names for the DataFrame\n",
        "columns = [f'pos{i}_coord{j}' for i in range(33) for j in range(2)]\n",
        "df_train.columns = columns\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df_train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7PNOfEPDn8z",
        "outputId": "cd4b533d-477a-4afb-d328-ed9dc8d009cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-94-1e4d36dec1fc>:7: DeprecationWarning: Please use `zoom` from the `scipy.ndimage` namespace, the `scipy.ndimage.interpolation` namespace is deprecated.\n",
            "  p_new[:, m, n] = inter.zoom(p[:, m, n], target_l / l)[:target_l]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mВыходные данные были обрезаны до нескольких последних строк (5000).\u001b[0m\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "(32, 33, 2)\n",
            "12256\n",
            "   pos0_coord0  pos0_coord1  pos1_coord0  pos1_coord1  pos2_coord0  \\\n",
            "0     0.431053     0.255754     0.429344     0.237740     0.430456   \n",
            "1     0.430692     0.261084     0.429278     0.248600     0.430019   \n",
            "2     0.434268     0.294032     0.433403     0.278151     0.434350   \n",
            "3     0.455328     0.307812     0.457614     0.290123     0.459554   \n",
            "4     0.446774     0.301506     0.448559     0.284090     0.450157   \n",
            "\n",
            "   pos2_coord1  pos3_coord0  pos3_coord1  pos4_coord0  pos4_coord1  ...  \\\n",
            "0     0.236740     0.432377     0.235841     0.429509     0.238156  ...   \n",
            "1     0.248790     0.431941     0.248964     0.431956     0.246020  ...   \n",
            "2     0.276948     0.436262     0.275621     0.435121     0.277872  ...   \n",
            "3     0.288513     0.461482     0.286610     0.454851     0.290862  ...   \n",
            "4     0.283285     0.451860     0.282345     0.448106     0.283687  ...   \n",
            "\n",
            "   pos28_coord0  pos28_coord1  pos29_coord0  pos29_coord1  pos30_coord0  \\\n",
            "0      0.486569      0.711372      0.537606      0.776412      0.499105   \n",
            "1      0.521649      0.764044      0.537488      0.789167      0.542949   \n",
            "2      0.518248      0.770929      0.538672      0.805068      0.537880   \n",
            "3      0.516797      0.767876      0.547463      0.790923      0.530495   \n",
            "4      0.514606      0.756731      0.538743      0.816001      0.526504   \n",
            "\n",
            "   pos30_coord1  pos31_coord0  pos31_coord1  pos32_coord0  pos32_coord1  \n",
            "0      0.697112      0.478348      0.800319      0.447854      0.733971  \n",
            "1      0.788224      0.477791      0.800630      0.484691      0.809611  \n",
            "2      0.796981      0.483316      0.804609      0.480040      0.807856  \n",
            "3      0.789766      0.518104      0.816746      0.472819      0.807981  \n",
            "4      0.761198      0.497674      0.814179      0.476553      0.786081  \n",
            "\n",
            "[5 rows x 66 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_train.tail())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yONaeUau3n_Y",
        "outputId": "978172d0-057b-4155-9903-c3a383e2985c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       pos0_coord0  pos0_coord1  pos1_coord0  pos1_coord1  pos2_coord0  \\\n",
            "12251     0.503593     0.169135     0.508932     0.151420     0.510543   \n",
            "12252     0.307091     0.161825     0.311640     0.145861     0.313125   \n",
            "12253     0.135274     0.152632     0.139560     0.134648     0.141142   \n",
            "12254    -0.017047     0.153857    -0.005760     0.134571    -0.002282   \n",
            "12255    -0.039782     0.105309    -0.024868     0.095427    -0.020878   \n",
            "\n",
            "       pos2_coord1  pos3_coord0  pos3_coord1  pos4_coord0  pos4_coord1  ...  \\\n",
            "12251     0.150991     0.512283     0.150644     0.508558     0.151559  ...   \n",
            "12252     0.145412     0.314747     0.145094     0.311178     0.146355  ...   \n",
            "12253     0.134480     0.142857     0.134525     0.139910     0.134173  ...   \n",
            "12254     0.133610     0.001242     0.132702    -0.012986     0.134570  ...   \n",
            "12255     0.095557    -0.016876     0.095750    -0.034295     0.095863  ...   \n",
            "\n",
            "       pos28_coord0  pos28_coord1  pos29_coord0  pos29_coord1  pos30_coord0  \\\n",
            "12251  4.739248e-01      0.798173  5.919481e-01      0.846770  4.791176e-01   \n",
            "12252  4.508335e-01      0.763677  3.084679e-01      0.827925  4.742016e-01   \n",
            "12253  1.075507e-01      0.762820  2.561838e-01      0.816282  1.105954e-01   \n",
            "12254  7.595568e-02      0.731407  1.066303e-01      0.755185  1.003642e-01   \n",
            "12255  1.447429e-17      0.669719  1.643988e-17      0.713988  1.428243e-17   \n",
            "\n",
            "       pos30_coord1  pos31_coord0  pos31_coord1  pos32_coord0  pos32_coord1  \n",
            "12251      0.823765  5.301022e-01      0.854424  4.171441e-01      0.827505  \n",
            "12252      0.794110  2.383682e-01      0.836208  4.381360e-01      0.818604  \n",
            "12253      0.786577  1.934075e-01      0.829842  5.264505e-02      0.798001  \n",
            "12254      0.743144  5.168585e-02      0.795539  5.702913e-02      0.787313  \n",
            "12255      0.688582  1.087827e-18      0.776477  8.928844e-18      0.737340  \n",
            "\n",
            "[5 rows x 66 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_videos = pickle.load(open(\"/content/drive/My Drive/KTH_Private/train.p\", \"rb\"))\n",
        "valid_videos = pickle.load(open(\"/content/drive/My Drive/KTH_Private/dev.p\", \"rb\"))\n",
        "test_videos = pickle.load(open(\"/content/drive/My Drive/KTH_Private/test.p\", \"rb\"))"
      ],
      "metadata": {
        "id": "4Rl5CmXBDuzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = []\n",
        "for video in train_videos:\n",
        "  train_labels.append(video['category'])\n",
        "print(len(train_labels))\n",
        "# train = train_poses\n",
        "\n",
        "\n",
        "val_labels = []\n",
        "for video in valid_videos:\n",
        "  val_labels.append(video['category'])\n",
        "print(len(val_labels))\n",
        "# val = val_poses\n",
        "\n",
        "test_labels = []\n",
        "for video in test_videos:\n",
        "  test_labels.append(video['category'])\n",
        "print(len(test_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udvn1_5xDzSc",
        "outputId": "8eac7600-0a86-4ff4-eb72-02094861002e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "191\n",
            "192\n",
            "216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_train_labels = []\n",
        "for a in train_labels:\n",
        "  if a == 'boxing':\n",
        "    new_train_labels.append(0)\n",
        "  elif a == 'handclapping':\n",
        "    new_train_labels.append(1)\n",
        "  elif a == 'handwaving':\n",
        "    new_train_labels.append(2)\n",
        "  elif a == 'jogging':\n",
        "    new_train_labels.append(3)\n",
        "  elif a == 'running':\n",
        "    new_train_labels.append(4)\n",
        "  else:\n",
        "    new_train_labels.append(5)\n",
        "\n",
        "print(new_train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wd4o4IAA9PWB",
        "outputId": "ccf94b59-9b4c-47ca-b44c-3342d072254b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_test_labels = []\n",
        "for a in test_labels:\n",
        "  if a == 'boxing':\n",
        "    new_test_labels.append(0)\n",
        "  elif a == 'handclapping':\n",
        "    new_test_labels.append(1)\n",
        "  elif a == 'handwaving':\n",
        "    new_test_labels.append(2)\n",
        "  elif a == 'jogging':\n",
        "    new_test_labels.append(3)\n",
        "  elif a == 'running':\n",
        "    new_test_labels.append(4)\n",
        "  else:\n",
        "    new_test_labels.append(5)\n",
        "\n",
        "print(new_test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdz3_Kmg98zg",
        "outputId": "2b45a96e-b71d-4779-9000-768ed62fdd92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = new_train_labels\n",
        "test_labels = new_test_labels"
      ],
      "metadata": {
        "id": "pDnD80fQEOhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_y = pd.DataFrame(train_labels)\n",
        "df_train_y.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "J4V0ZNRCD_VD",
        "outputId": "ac534172-1ab6-476a-847a-46482fc21051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   0\n",
              "0  0\n",
              "1  0\n",
              "2  0\n",
              "3  0\n",
              "4  0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-75e639e8-eb35-4982-b9af-1945c4770613\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-75e639e8-eb35-4982-b9af-1945c4770613')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-75e639e8-eb35-4982-b9af-1945c4770613 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-75e639e8-eb35-4982-b9af-1945c4770613');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example data\n",
        "\n",
        "data = []\n",
        "for video in test:\n",
        "  video  = video[::10]\n",
        "  video = zoom(video)\n",
        "  data.append(video)\n",
        "number_Of_videos = len(data)\n",
        "\n",
        "# Reshape and concatenate the data for each frame of each video\n",
        "frames = []\n",
        "for i in range(number_Of_videos):\n",
        "    for j in range(0, data[i].shape[0]):\n",
        "        reshaped_data = data[i][j].reshape((1, 33*2))\n",
        "        frames.append(pd.DataFrame(data=reshaped_data))\n",
        "\n",
        "# Concatenate the data frames into a single DataFrame\n",
        "df_test = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "# Create column names for the DataFrame\n",
        "columns = [f'pos{i}_coord{j}' for i in range(33) for j in range(2)]\n",
        "df_test.columns = columns\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(df_test.head())\n",
        "\n",
        "\n",
        "df_test_y = pd.DataFrame(test_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpRmAJnXENfN",
        "outputId": "909d1b80-9cc3-4e9d-97de-c2f94241a870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-94-1e4d36dec1fc>:7: DeprecationWarning: Please use `zoom` from the `scipy.ndimage` namespace, the `scipy.ndimage.interpolation` namespace is deprecated.\n",
            "  p_new[:, m, n] = inter.zoom(p[:, m, n], target_l / l)[:target_l]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   pos0_coord0  pos0_coord1  pos1_coord0  pos1_coord1  pos2_coord0  \\\n",
            "0     0.463389     0.155386     0.469397     0.138264     0.470773   \n",
            "1     0.480198     0.154725     0.483175     0.137574     0.484967   \n",
            "2     0.481175     0.162920     0.487593     0.146236     0.488994   \n",
            "3     0.484946     0.180979     0.488111     0.166438     0.489493   \n",
            "4     0.495260     0.162451     0.502201     0.146910     0.504344   \n",
            "\n",
            "   pos2_coord1  pos3_coord0  pos3_coord1  pos4_coord0  pos4_coord1  ...  \\\n",
            "0     0.137981     0.472343     0.137717     0.470786     0.137522  ...   \n",
            "1     0.137332     0.486926     0.137107     0.483783     0.136793  ...   \n",
            "2     0.145567     0.490587     0.144919     0.488047     0.145846  ...   \n",
            "3     0.165833     0.491111     0.165152     0.488608     0.165715  ...   \n",
            "4     0.146281     0.506541     0.145632     0.501706     0.146364  ...   \n",
            "\n",
            "   pos28_coord0  pos28_coord1  pos29_coord0  pos29_coord1  pos30_coord0  \\\n",
            "0      0.526239      0.767981      0.559087      0.875316      0.547473   \n",
            "1      0.542213      0.771535      0.561391      0.875972      0.550928   \n",
            "2      0.542442      0.771493      0.567866      0.879045      0.552743   \n",
            "3      0.543151      0.767047      0.570103      0.879276      0.554785   \n",
            "4      0.549648      0.766198      0.579728      0.868618      0.559304   \n",
            "\n",
            "   pos30_coord1  pos31_coord0  pos31_coord1  pos32_coord0  pos32_coord1  \n",
            "0      0.795262      0.481648      0.874059      0.468578      0.781359  \n",
            "1      0.795407      0.484993      0.873540      0.485310      0.789356  \n",
            "2      0.793826      0.488341      0.879513      0.483195      0.787589  \n",
            "3      0.791701      0.497712      0.881251      0.486773      0.787668  \n",
            "4      0.784206      0.504397      0.865493      0.497699      0.784101  \n",
            "\n",
            "[5 rows x 66 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YdsI8D87hEO",
        "outputId": "d3878040-0b3d-43da-cbf7-c35ce2d3d478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LABELS = ['boxing' 'handclapping', 'handwaving', 'jogging', 'running', 'walking']"
      ],
      "metadata": {
        "id": "OB4WcGM-ARMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZ0RvCtPaI8B"
      },
      "outputs": [],
      "source": [
        "class Math():\n",
        "    ''' Some math operations '''\n",
        "    @staticmethod\n",
        "    def calc_dist(p1, p0):\n",
        "        return math.sqrt((p1[0]-p0[0])**2+(p1[1]-p0[1])**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def pi2pi(x):\n",
        "        if x > PI:\n",
        "            x -= 2*PI\n",
        "        if x <= -PI:\n",
        "            x += 2*PI\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def calc_relative_angle(x1, y1, x0, y0, base_angle):\n",
        "        # compute rotation from {base_angle} to {(x0,y0)->(x1,y1)}\n",
        "        if (y1 == y0) and (x1 == x0):\n",
        "            return 0\n",
        "        a1 = np.arctan2(y1-y0, x1-x0)\n",
        "        return Math.pi2pi(a1 - base_angle)\n",
        "\n",
        "    @staticmethod\n",
        "    def calc_relative_angle_v2(p1, p0, base_angle):\n",
        "        # compute rotation from {base_angle} to {p0->p1}\n",
        "        return Math.calc_relative_angle(p1[0], p1[1], p0[0], p0[1], base_angle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiaCYDtLaI8C"
      },
      "outputs": [],
      "source": [
        "# -- Functions for processing features\n",
        "def get_joint(x, idx):\n",
        "    px = x[2*idx]\n",
        "    py = x[2*idx+1]\n",
        "    return px, py\n",
        "\n",
        "def set_joint(x, idx, px, py):\n",
        "    x[2*idx] = px\n",
        "    x[2*idx+1] = py\n",
        "    return\n",
        "\n",
        "def check_joint(x, idx):\n",
        "    return x[2*idx] != NaN\n",
        "\n",
        "class ProcFtr(object):\n",
        "\n",
        "    @staticmethod\n",
        "    def drop_arms_and_legs_randomly(x, thresh=0.3):\n",
        "        ''' Randomly drop one arm or one leg with a probability of thresh '''\n",
        "        x = x.copy()\n",
        "        N = len(ARMS_LEGS)\n",
        "        rand_num = np.random.random()\n",
        "        if rand_num < thresh:\n",
        "            joint_idx = int((rand_num / thresh)*N)\n",
        "            set_joint(x, joint_idx, NaN, NaN)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def has_neck_and_thigh(x):\n",
        "        ''' Check if a skeleton has a neck and at least one thigh '''\n",
        "        return check_joint(x, NECK) and (check_joint(x, L_THIGH) or check_joint(x, R_THIGH))\n",
        "\n",
        "    @staticmethod\n",
        "    def get_body_height(x):\n",
        "        ''' Compute height of the body, which is defined as:\n",
        "            the distance between `neck` and `thigh`.\n",
        "        '''\n",
        "        x0, y0 = get_joint(x, NECK)\n",
        "\n",
        "        # Get average thigh height\n",
        "        x11, y11 = get_joint(x, L_THIGH)\n",
        "        x12, y12 = get_joint(x, R_THIGH)\n",
        "        if y11 == NaN and y12 == NaN:  # Invalid data\n",
        "            return 1.0\n",
        "        if y11 == NaN:\n",
        "            x1, y1 = x12, y12\n",
        "        elif y12 == NaN:\n",
        "            x1, y1 = x11, y11\n",
        "        else:\n",
        "            x1, y1 = (x11 + x12) / 2, (y11 + y12) / 2\n",
        "\n",
        "        # Get body height\n",
        "        height = ((x0-x1)**2 + (y0-y1)**2)**(0.5)\n",
        "        return height\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_body_offset(x):\n",
        "        ''' The origin is the neck.\n",
        "        TODO: Deal with empty data.\n",
        "        '''\n",
        "        x = x.copy()\n",
        "        px0, py0 = get_joint(x, NECK)\n",
        "        x[0::2] = x[0::2] - px0\n",
        "        x[1::2] = x[1::2] - py0\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def joint_pos_2_angle_and_length(x):\n",
        "        ''' Change the representation of skeletons\n",
        "            From xy positions to angle and length.\n",
        "        '''\n",
        "        # ---------------------- Get joint positions ----------------------\n",
        "        class JointPosExtractor(object):\n",
        "            def __init__(self, x):\n",
        "                self.x = x\n",
        "                self.i = 0\n",
        "\n",
        "            def get_next_point(self):\n",
        "                p = [self.x[self.i], self.x[self.i+1]]\n",
        "                self.i += 2\n",
        "                return p\n",
        "        tmp = JointPosExtractor(x)\n",
        "\n",
        "        pneck = tmp.get_next_point()\n",
        "\n",
        "        prshoulder = tmp.get_next_point()\n",
        "        prelbow = tmp.get_next_point()\n",
        "        prwrist = tmp.get_next_point()\n",
        "\n",
        "        plshoulder = tmp.get_next_point()\n",
        "        plelbow = tmp.get_next_point()\n",
        "        plwrist = tmp.get_next_point()\n",
        "\n",
        "        prhip = tmp.get_next_point()\n",
        "        prknee = tmp.get_next_point()\n",
        "        prankle = tmp.get_next_point()\n",
        "\n",
        "        plhip = tmp.get_next_point()\n",
        "        plknee = tmp.get_next_point()\n",
        "        plankle = tmp.get_next_point()\n",
        "\n",
        "        # ---------------------- Get joint angels ----------------------\n",
        "        class Get12Angles(object):\n",
        "            def __init__(self):\n",
        "                self.j = 0\n",
        "                self.f_angles = np.zeros((12,))\n",
        "                self.x_lengths = np.zeros((12,))\n",
        "\n",
        "            def set_next_angle_len(self, next_joint, base_joint, base_angle):\n",
        "                angle = Math.calc_relative_angle_v2(\n",
        "                    next_joint, base_joint, base_angle)\n",
        "                dist = Math.calc_dist(next_joint, base_joint)\n",
        "                self.f_angles[self.j] = angle\n",
        "                self.x_lengths[self.j] = dist\n",
        "                self.j += 1\n",
        "\n",
        "        tmp2 = Get12Angles()\n",
        "        tmp2.set_next_angle_len(prshoulder, pneck, PI)  # r-shoulder\n",
        "        tmp2.set_next_angle_len(prelbow, prshoulder, PI/2)  # r-elbow\n",
        "        tmp2.set_next_angle_len(prwrist, prelbow, PI/2)  # r-wrist\n",
        "\n",
        "        tmp2.set_next_angle_len(plshoulder, pneck, 0)  # l-shoulder\n",
        "        tmp2.set_next_angle_len(plelbow, plshoulder, PI/2)  # l-elbow\n",
        "        tmp2.set_next_angle_len(plwrist, plelbow, PI/2)  # l-wrist\n",
        "\n",
        "        tmp2.set_next_angle_len(prhip, pneck, PI/2+PI/18)\n",
        "        tmp2.set_next_angle_len(prknee, prhip, PI/2)\n",
        "        tmp2.set_next_angle_len(prankle, prknee, PI/2)\n",
        "\n",
        "        tmp2.set_next_angle_len(plhip, pneck, PI/2-PI/18)\n",
        "        tmp2.set_next_angle_len(plknee, plhip, PI/2)\n",
        "        tmp2.set_next_angle_len(plankle, plknee, PI/2)\n",
        "\n",
        "        # Output\n",
        "        features_angles = tmp2.f_angles\n",
        "        features_lens = tmp2.x_lengths\n",
        "        return features_angles, features_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature generator class:\n",
        "1. Fills missing data with its relative position in previous frame\n",
        "2. Generates normalized skeleton joints feature\n",
        "3. Computes velocity of center feature\n",
        "4. Computes velocity of joints feature\n",
        "5. Computes euclidean distance of pairs of joints"
      ],
      "metadata": {
        "id": "bFPfomuYRber"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQ7w-OxCaI8D"
      },
      "outputs": [],
      "source": [
        "# -- The main class for extracting features\n",
        "class FeatureGenerator(object):\n",
        "    def __init__(self,\n",
        "                 window_size,\n",
        "                 is_adding_noise=False):\n",
        "        '''\n",
        "        Arguments:\n",
        "            window_size {int}: Number of adjacent frames for extracting features.\n",
        "            is_adding_noise {bool}: Is adding noise to the joint positions and scale.\n",
        "            noise_intensity {float}: The noise relative to the body height.\n",
        "        '''\n",
        "        self._window_size = window_size\n",
        "        self._is_adding_noise = is_adding_noise\n",
        "        self._noise_intensity = NOISE_INTENSITY\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        ''' Reset the FeatureGenerator '''\n",
        "        self._x_deque = deque()\n",
        "        self._angles_deque = deque()\n",
        "        self._lens_deque = deque()\n",
        "        self._pre_x = None\n",
        "\n",
        "    def add_cur_skeleton(self, skeleton):\n",
        "        ''' Input a new skeleton, return the extracted feature.\n",
        "        Returns:\n",
        "            is_success {bool}: Return the feature only when\n",
        "                the historical input skeletons are more than self._window_size.\n",
        "            features {np.array}\n",
        "        '''\n",
        "        x = retrain_only_body_joints(skeleton)\n",
        "\n",
        "#         if not ProcFtr.has_neck_and_thigh(x):\n",
        "#             self.reset()\n",
        "#             print('False', x)\n",
        "#             return False, None\n",
        "#         else:\n",
        "        ''' The input skeleton has a neck and at least one thigh '''\n",
        "            # -- Preprocess x\n",
        "            # Fill zeros, compute angles/lens\n",
        "        x = self._fill_invalid_data(x)\n",
        "        if self._is_adding_noise:\n",
        "                # Add noise druing training stage to augment data\n",
        "            x = self._add_noises(x, self._noise_intensity)\n",
        "        x = np.array(x)\n",
        "        # angles, lens = ProcFtr.joint_pos_2_angle_and_length(x) # deprecate\n",
        "\n",
        "            # Push to deque\n",
        "        self._x_deque.append(x)\n",
        "        # self._angles_deque.append(angles) # deprecate\n",
        "        # self._lens_deque.append(lens) # deprecate\n",
        "\n",
        "        self._maintain_deque_size()\n",
        "        self._pre_x = x.copy()\n",
        "\n",
        "            # -- Extract features\n",
        "        if len(self._x_deque) < self._window_size:\n",
        "            return False, None, None\n",
        "\n",
        "        else:\n",
        "                # -- Normalize all 1~t features\n",
        "            h_list = [ProcFtr.get_body_height(xi) for xi in self._x_deque]\n",
        "            mean_height = np.mean(h_list)\n",
        "            xnorm_list = [ProcFtr.remove_body_offset(xi)/mean_height\n",
        "                            for xi in self._x_deque]\n",
        "\n",
        "                # -- Get features of pose/angles/lens\n",
        "            f_poses = self._deque_features_to_1darray(xnorm_list)\n",
        "            # f_angles = self._deque_features_to_1darray(self._angles_deque) # deprecate\n",
        "            # f_lens = self._deque_features_to_1darray(\n",
        "            #         self._lens_deque) / mean_height # deprecate\n",
        "\n",
        "                #  Get features of motion\n",
        "\n",
        "            f_v_center = self._compute_v_center(\n",
        "                self._x_deque, step=1) / mean_height  # len = (t=4)*2 = 8\n",
        "            f_v_center = np.repeat(f_v_center, 10)  # repeat to add weight\n",
        "\n",
        "            f_v_joints = self._compute_v_all_joints(\n",
        "                xnorm_list, step=1)  # len = (t=(5-1)/step)*12*2 = 96\n",
        "            f_d_joints = self._compute_distance(xnorm_list)\n",
        "\n",
        "                # -- Output\n",
        "            features = np.concatenate((f_poses, f_v_joints, f_v_center))\n",
        "\n",
        "            distance = f_d_joints\n",
        "            return True, features.copy(), distance.copy()\n",
        "\n",
        "    def _maintain_deque_size(self):\n",
        "        if len(self._x_deque) > self._window_size:\n",
        "            self._x_deque.popleft()\n",
        "        if len(self._angles_deque) > self._window_size:\n",
        "            self._angles_deque.popleft()\n",
        "        if len(self._lens_deque) > self._window_size:\n",
        "            self._lens_deque.popleft()\n",
        "\n",
        "    def _compute_v_center(self, x_deque, step):\n",
        "        vel = []\n",
        "        for i in range(0, len(x_deque) - step, step):\n",
        "            dxdy = x_deque[i+step][0:2] - x_deque[i][0:2]\n",
        "            vel += dxdy.tolist()\n",
        "        return np.array(vel)\n",
        "\n",
        "    def _compute_v_all_joints(self, xnorm_list, step):\n",
        "        vel = []\n",
        "        for i in range(0, len(xnorm_list) - step, step):\n",
        "            dxdy = xnorm_list[i+step][:] - xnorm_list[i][:]\n",
        "            vel += dxdy.tolist()\n",
        "        return np.array(vel)\n",
        "\n",
        "    def _compute_distance(self, xnorm_list):\n",
        "        dist = []\n",
        "        for i in range(0, len(xnorm_list)):\n",
        "            x = np.reshape(xnorm_list[i], (33, 2))\n",
        "            dist_arr = pdist(x)\n",
        "            dist+= dist_arr.tolist()\n",
        "        #print('dist', len(dist))\n",
        "        return np.array(dist)\n",
        "\n",
        "    def _fill_invalid_data(self, x):\n",
        "        ''' Fill the NaN elements in x with\n",
        "            their relative-to-neck position in the preious x.\n",
        "        Argument:\n",
        "            x {np.array}: a skeleton that has a neck and at least a thigh.\n",
        "        '''\n",
        "        res = x.copy()\n",
        "\n",
        "        def get_px_py_px0_py0(x):\n",
        "            px = x[0::2]  # list of x\n",
        "            py = x[1::2]  # list of y\n",
        "            px0, py0 = get_joint(x, NECK)  # neck\n",
        "            return px, py, px0, py0\n",
        "        cur_px, cur_py, cur_px0, cur_py0 = get_px_py_px0_py0(x)\n",
        "        cur_height = ProcFtr.get_body_height(x)\n",
        "\n",
        "        is_lack_knee = check_joint(x, L_KNEE) or check_joint(x, R_KNEE)\n",
        "        is_lack_ankle = check_joint(x, L_ANKLE) or check_joint(x, R_ANKLE)\n",
        "        if (self._pre_x is None) or is_lack_knee or is_lack_ankle:\n",
        "            # If preious data is invalid or there is no knee or ankle,\n",
        "            # then fill the data based on the STAND_SKEL_NORMED.\n",
        "            for i in range(TOTAL_JOINTS*2):\n",
        "                if res[i] == NaN:\n",
        "                    res[i] = (cur_px0 if i % 2 == 0 else cur_py0) + \\\n",
        "                        cur_height * STAND_SKEL_NORMED[i]\n",
        "            return res\n",
        "\n",
        "        pre_px, pre_py, pre_px0, pre_py0 = get_px_py_px0_py0(self._pre_x)\n",
        "        pre_height = ProcFtr.get_body_height(self._pre_x)\n",
        "\n",
        "        scale = cur_height / pre_height\n",
        "\n",
        "        bad_idxs = np.nonzero(cur_px == NaN)[0]\n",
        "        if not len(bad_idxs):  # No invalid data\n",
        "            return res\n",
        "\n",
        "        cur_px[bad_idxs] = cur_px0 + (pre_px[bad_idxs] - pre_px0) * scale\n",
        "        cur_py[bad_idxs] = cur_py0 + (pre_py[bad_idxs] - pre_py0) * scale\n",
        "        res[::2] = cur_px\n",
        "        res[1::2] = cur_py\n",
        "        return res\n",
        "\n",
        "\n",
        "    def _add_noises(self, x, intensity):\n",
        "        ''' Add noise to x with a ratio relative to the body height '''\n",
        "        height = ProcFtr.get_body_height(x)\n",
        "        randoms = (np.random.random(x.shape, ) - 0.5) * 2 * intensity * height\n",
        "        x = [(xi + randoms[i] if xi != 0 else xi)\n",
        "             for i, xi in enumerate(x)]\n",
        "        return x\n",
        "\n",
        "    def _deque_features_to_1darray(self, deque_data):\n",
        "        features = []\n",
        "        for i in range(len(deque_data)):\n",
        "            next_feature = deque_data[i].tolist()\n",
        "            features += next_feature\n",
        "        features = np.array(features)\n",
        "        return features\n",
        "\n",
        "    def _deque_features_to_2darray(self, deque_data):\n",
        "        features = []\n",
        "        for i in range(len(deque_data)):\n",
        "            next_feature = deque_data[i].tolist()\n",
        "            features.append(next_feature)\n",
        "        features = np.array(features)\n",
        "        return features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positions of skeleton joints"
      ],
      "metadata": {
        "id": "85foYowZTAfH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czskOStMaI8J"
      },
      "outputs": [],
      "source": [
        "# -- Settings\n",
        "NOISE_INTENSITY = 0.05\n",
        "# -- Constant\n",
        "PI = np.pi\n",
        "Inf = float(\"inf\")\n",
        "NaN = 0.0\n",
        "\n",
        "def retrain_only_body_joints(skeleton):\n",
        "    ''' All skeleton operations in this script are done after this function.\n",
        "    '''\n",
        "    return skeleton.copy()\n",
        "\n",
        "TOTAL_JOINTS = 15\n",
        "NECK = 0\n",
        "Belly = 1\n",
        "HEAD = 2\n",
        "L_ARMS = [3, 7, 11]\n",
        "R_ARMS = [4, 8, 12]\n",
        "L_KNEE = 9\n",
        "L_ANKLE = 13\n",
        "R_KNEE = 10\n",
        "R_ANKLE = 14\n",
        "L_LEGS = [9, 13]\n",
        "R_LEGS = [10, 14]\n",
        "ARMS_LEGS = L_ARMS + R_ARMS + L_LEGS + R_LEGS\n",
        "L_THIGH = 5\n",
        "R_THIGH = 6\n",
        "\n",
        "STAND_SKEL_NORMED = retrain_only_body_joints(\n",
        "    get_a_normalized_jhmdb_skeleton())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3Qq6ATraI8K"
      },
      "outputs": [],
      "source": [
        "def process_features(X0, Y0, video_indices, classes):\n",
        "    ''' Process features '''\n",
        "    # Convert features\n",
        "    # From: raw feature of individual image.\n",
        "    # To:   time-serials features calculated from multiple raw features\n",
        "    #       of multiple adjacent images, including velocity, normalized pos, etc.\n",
        "    ADD_NOISE = False\n",
        "    if ADD_NOISE:\n",
        "        X1, Y1, X_distance1= extract_multi_frame_features(\n",
        "            X0, Y0, video_indices, WINDOW_SIZE,\n",
        "            is_adding_noise=True, is_print=True)\n",
        "        X2, Y2, X_distance2 = extract_multi_frame_features(\n",
        "            X0, Y0, video_indices, WINDOW_SIZE,\n",
        "            is_adding_noise=False, is_print=True)\n",
        "        X = np.vstack((X1, X2))\n",
        "        Y = np.concatenate((Y1, Y2))\n",
        "        X_distance = np.vstack((X_distance1, X_distance2))\n",
        "        return X, Y, X_distance\n",
        "    else:\n",
        "        X, Y, X_distance = extract_multi_frame_features(\n",
        "            X0, Y0, video_indices, WINDOW_SIZE,\n",
        "            is_adding_noise=False, is_print=True)\n",
        "        return X, Y, X_distance\n",
        "# -- Functions\n",
        "def extract_multi_frame_features(\n",
        "        X, Y, video_indices, window_size,\n",
        "        is_adding_noise=False, is_print=False):\n",
        "    ''' From image index and raw skeleton positions,\n",
        "        Extract features of body velocity, joint velocity, and normalized joint positions, distance feature.\n",
        "    '''\n",
        "    X_new = []\n",
        "    Y_new = []\n",
        "    N = len(video_indices)\n",
        "    X_distance = []\n",
        "\n",
        "    # Loop through all data\n",
        "\n",
        "    for i, _ in enumerate(video_indices):\n",
        "\n",
        "        # If a new video clip starts, reset the feature generator\n",
        "        if i == 0 or video_indices[i] != video_indices[i-1]:\n",
        "            fg = FeatureGenerator(window_size, is_adding_noise)\n",
        "\n",
        "        # Get features\n",
        "        success, features, distance = fg.add_cur_skeleton(X[i, :])\n",
        "        if success:  # True if (data length > 5) and (skeleton has enough joints)\n",
        "            X_new.append(features)\n",
        "            Y_new.append(Y[i])\n",
        "            X_distance.append(distance)\n",
        "\n",
        "        # Print\n",
        "        if is_print and i % 1000 == 0:\n",
        "            print(f\"{i}/{N}\", end=\", \")\n",
        "\n",
        "    if is_print:\n",
        "        print(\"\")\n",
        "    X_new = np.array(X_new)\n",
        "    Y_new = np.array(Y_new)\n",
        "    X_distance = np.array(X_distance)\n",
        "    return X_new, Y_new, X_distance\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading"
      ],
      "metadata": {
        "id": "n_bibKaDW1qV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "JHMDB data has three splits and each of them splited into train and test set. Each set contains skeleton dataset and labels dataset.\n",
        "\n",
        "Skeleton data in csv format and each row contains skeleton joint coordinates of one frame.\n",
        "\n",
        "Dataset of labels contains action labels of each video."
      ],
      "metadata": {
        "id": "jIiWVKmET6rK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jge09JckaI8U"
      },
      "source": [
        "# Split 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QyjxDlv73eWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train+val"
      ],
      "metadata": {
        "id": "2a9lUrMkWcY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhlKgAZIaI8X"
      },
      "outputs": [],
      "source": [
        "x_1 = pd.read_csv(path+'jhmdb/data/GT_train_1.csv', header=None)\n",
        "y_1 = pd.read_csv(path+'jhmdb/data/GT_train_y_1.csv', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "fo7-vmfVu9Q8",
        "outputId": "e25835cd-8118-4887-9bdd-db61928c97d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       pos0_coord0  pos0_coord1  pos1_coord0  pos1_coord1  pos2_coord0  \\\n",
              "12251     0.503593     0.169135     0.508932     0.151420     0.510543   \n",
              "12252     0.307091     0.161825     0.311640     0.145861     0.313125   \n",
              "12253     0.135274     0.152632     0.139560     0.134648     0.141142   \n",
              "12254    -0.017047     0.153857    -0.005760     0.134571    -0.002282   \n",
              "12255    -0.039782     0.105309    -0.024868     0.095427    -0.020878   \n",
              "\n",
              "       pos2_coord1  pos3_coord0  pos3_coord1  pos4_coord0  pos4_coord1  ...  \\\n",
              "12251     0.150991     0.512283     0.150644     0.508558     0.151559  ...   \n",
              "12252     0.145412     0.314747     0.145094     0.311178     0.146355  ...   \n",
              "12253     0.134480     0.142857     0.134525     0.139910     0.134173  ...   \n",
              "12254     0.133610     0.001242     0.132702    -0.012986     0.134570  ...   \n",
              "12255     0.095557    -0.016876     0.095750    -0.034295     0.095863  ...   \n",
              "\n",
              "       pos28_coord0  pos28_coord1  pos29_coord0  pos29_coord1  pos30_coord0  \\\n",
              "12251  4.739248e-01      0.798173  5.919481e-01      0.846770  4.791176e-01   \n",
              "12252  4.508335e-01      0.763677  3.084679e-01      0.827925  4.742016e-01   \n",
              "12253  1.075507e-01      0.762820  2.561838e-01      0.816282  1.105954e-01   \n",
              "12254  7.595568e-02      0.731407  1.066303e-01      0.755185  1.003642e-01   \n",
              "12255  1.447429e-17      0.669719  1.643988e-17      0.713988  1.428243e-17   \n",
              "\n",
              "       pos30_coord1  pos31_coord0  pos31_coord1  pos32_coord0  pos32_coord1  \n",
              "12251      0.823765  5.301022e-01      0.854424  4.171441e-01      0.827505  \n",
              "12252      0.794110  2.383682e-01      0.836208  4.381360e-01      0.818604  \n",
              "12253      0.786577  1.934075e-01      0.829842  5.264505e-02      0.798001  \n",
              "12254      0.743144  5.168585e-02      0.795539  5.702913e-02      0.787313  \n",
              "12255      0.688582  1.087827e-18      0.776477  8.928844e-18      0.737340  \n",
              "\n",
              "[5 rows x 66 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9ba1c8ae-97ed-4e8a-8e53-8cbc48b81f19\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pos0_coord0</th>\n",
              "      <th>pos0_coord1</th>\n",
              "      <th>pos1_coord0</th>\n",
              "      <th>pos1_coord1</th>\n",
              "      <th>pos2_coord0</th>\n",
              "      <th>pos2_coord1</th>\n",
              "      <th>pos3_coord0</th>\n",
              "      <th>pos3_coord1</th>\n",
              "      <th>pos4_coord0</th>\n",
              "      <th>pos4_coord1</th>\n",
              "      <th>...</th>\n",
              "      <th>pos28_coord0</th>\n",
              "      <th>pos28_coord1</th>\n",
              "      <th>pos29_coord0</th>\n",
              "      <th>pos29_coord1</th>\n",
              "      <th>pos30_coord0</th>\n",
              "      <th>pos30_coord1</th>\n",
              "      <th>pos31_coord0</th>\n",
              "      <th>pos31_coord1</th>\n",
              "      <th>pos32_coord0</th>\n",
              "      <th>pos32_coord1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12251</th>\n",
              "      <td>0.503593</td>\n",
              "      <td>0.169135</td>\n",
              "      <td>0.508932</td>\n",
              "      <td>0.151420</td>\n",
              "      <td>0.510543</td>\n",
              "      <td>0.150991</td>\n",
              "      <td>0.512283</td>\n",
              "      <td>0.150644</td>\n",
              "      <td>0.508558</td>\n",
              "      <td>0.151559</td>\n",
              "      <td>...</td>\n",
              "      <td>4.739248e-01</td>\n",
              "      <td>0.798173</td>\n",
              "      <td>5.919481e-01</td>\n",
              "      <td>0.846770</td>\n",
              "      <td>4.791176e-01</td>\n",
              "      <td>0.823765</td>\n",
              "      <td>5.301022e-01</td>\n",
              "      <td>0.854424</td>\n",
              "      <td>4.171441e-01</td>\n",
              "      <td>0.827505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12252</th>\n",
              "      <td>0.307091</td>\n",
              "      <td>0.161825</td>\n",
              "      <td>0.311640</td>\n",
              "      <td>0.145861</td>\n",
              "      <td>0.313125</td>\n",
              "      <td>0.145412</td>\n",
              "      <td>0.314747</td>\n",
              "      <td>0.145094</td>\n",
              "      <td>0.311178</td>\n",
              "      <td>0.146355</td>\n",
              "      <td>...</td>\n",
              "      <td>4.508335e-01</td>\n",
              "      <td>0.763677</td>\n",
              "      <td>3.084679e-01</td>\n",
              "      <td>0.827925</td>\n",
              "      <td>4.742016e-01</td>\n",
              "      <td>0.794110</td>\n",
              "      <td>2.383682e-01</td>\n",
              "      <td>0.836208</td>\n",
              "      <td>4.381360e-01</td>\n",
              "      <td>0.818604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12253</th>\n",
              "      <td>0.135274</td>\n",
              "      <td>0.152632</td>\n",
              "      <td>0.139560</td>\n",
              "      <td>0.134648</td>\n",
              "      <td>0.141142</td>\n",
              "      <td>0.134480</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.134525</td>\n",
              "      <td>0.139910</td>\n",
              "      <td>0.134173</td>\n",
              "      <td>...</td>\n",
              "      <td>1.075507e-01</td>\n",
              "      <td>0.762820</td>\n",
              "      <td>2.561838e-01</td>\n",
              "      <td>0.816282</td>\n",
              "      <td>1.105954e-01</td>\n",
              "      <td>0.786577</td>\n",
              "      <td>1.934075e-01</td>\n",
              "      <td>0.829842</td>\n",
              "      <td>5.264505e-02</td>\n",
              "      <td>0.798001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12254</th>\n",
              "      <td>-0.017047</td>\n",
              "      <td>0.153857</td>\n",
              "      <td>-0.005760</td>\n",
              "      <td>0.134571</td>\n",
              "      <td>-0.002282</td>\n",
              "      <td>0.133610</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.132702</td>\n",
              "      <td>-0.012986</td>\n",
              "      <td>0.134570</td>\n",
              "      <td>...</td>\n",
              "      <td>7.595568e-02</td>\n",
              "      <td>0.731407</td>\n",
              "      <td>1.066303e-01</td>\n",
              "      <td>0.755185</td>\n",
              "      <td>1.003642e-01</td>\n",
              "      <td>0.743144</td>\n",
              "      <td>5.168585e-02</td>\n",
              "      <td>0.795539</td>\n",
              "      <td>5.702913e-02</td>\n",
              "      <td>0.787313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12255</th>\n",
              "      <td>-0.039782</td>\n",
              "      <td>0.105309</td>\n",
              "      <td>-0.024868</td>\n",
              "      <td>0.095427</td>\n",
              "      <td>-0.020878</td>\n",
              "      <td>0.095557</td>\n",
              "      <td>-0.016876</td>\n",
              "      <td>0.095750</td>\n",
              "      <td>-0.034295</td>\n",
              "      <td>0.095863</td>\n",
              "      <td>...</td>\n",
              "      <td>1.447429e-17</td>\n",
              "      <td>0.669719</td>\n",
              "      <td>1.643988e-17</td>\n",
              "      <td>0.713988</td>\n",
              "      <td>1.428243e-17</td>\n",
              "      <td>0.688582</td>\n",
              "      <td>1.087827e-18</td>\n",
              "      <td>0.776477</td>\n",
              "      <td>8.928844e-18</td>\n",
              "      <td>0.737340</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 66 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ba1c8ae-97ed-4e8a-8e53-8cbc48b81f19')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9ba1c8ae-97ed-4e8a-8e53-8cbc48b81f19 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9ba1c8ae-97ed-4e8a-8e53-8cbc48b81f19');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = train_labels + val_labels"
      ],
      "metadata": {
        "id": "Wre5rU-2WU4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_y = pd.DataFrame(train_labels)\n",
        "df_train_y.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "aemtSvQj5JZW",
        "outputId": "14683f6d-0e4c-4c21-eb7f-d849881c0252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   0\n",
              "0  0\n",
              "1  0\n",
              "2  0\n",
              "3  0\n",
              "4  0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a436ca3-7fc1-4d0c-9dea-9fe7e42ede2a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a436ca3-7fc1-4d0c-9dea-9fe7e42ede2a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8a436ca3-7fc1-4d0c-9dea-9fe7e42ede2a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8a436ca3-7fc1-4d0c-9dea-9fe7e42ede2a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_y.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HW-8aSs45JcI",
        "outputId": "12c3f451-0846-437f-bbbf-7761eeb762d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     0\n",
              "378  5\n",
              "379  5\n",
              "380  5\n",
              "381  5\n",
              "382  5"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-04a747e2-49f2-41a8-a714-734b43ee57d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>381</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>382</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-04a747e2-49f2-41a8-a714-734b43ee57d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-04a747e2-49f2-41a8-a714-734b43ee57d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-04a747e2-49f2-41a8-a714-734b43ee57d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "draw(df_train.iloc[70])\n",
        "df_train[df_train.columns[1::2]] = df_train[df_train.columns[1::2]].apply(lambda y: -y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "RT6No9YOLILO",
        "outputId": "9ae669f4-1afc-4ebf-b287-312d7391b940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "draw(df_train.iloc[70])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "KywKnMZS2zOK",
        "outputId": "1f82196b-2f5e-4f95-ca31-67c28678a722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "draw(df_test.iloc[90])\n",
        "\n",
        "df_test[df_test.columns[1::2]] = df_test[df_test.columns[1::2]].apply(lambda y: -y)\n",
        "draw(df_test.iloc[90])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "id": "ipRjdpBILnhl",
        "outputId": "e7187a24-cfc4-484b-db59-1c2ab8af7a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_1.iloc[15315])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "sINRTiwnt_z0",
        "outputId": "8d4aab80-d40f-4553-c60b-f9e50207a29a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-117-d187625c450e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15315\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'x_1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for a in train:\n",
        "  print(a.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae_kVD3K-_ja",
        "outputId": "1554f9e9-a353-465a-cf5e-56731e452522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(350, 33, 2)\n",
            "(378, 33, 2)\n",
            "(443, 33, 2)\n",
            "(249, 33, 2)\n",
            "(589, 33, 2)\n",
            "(659, 33, 2)\n",
            "(691, 33, 2)\n",
            "(483, 33, 2)\n",
            "(537, 33, 2)\n",
            "(604, 33, 2)\n",
            "(418, 33, 2)\n",
            "(419, 33, 2)\n",
            "(557, 33, 2)\n",
            "(557, 33, 2)\n",
            "(561, 33, 2)\n",
            "(434, 33, 2)\n",
            "(407, 33, 2)\n",
            "(484, 33, 2)\n",
            "(417, 33, 2)\n",
            "(293, 33, 2)\n",
            "(493, 33, 2)\n",
            "(373, 33, 2)\n",
            "(529, 33, 2)\n",
            "(459, 33, 2)\n",
            "(377, 33, 2)\n",
            "(489, 33, 2)\n",
            "(471, 33, 2)\n",
            "(341, 33, 2)\n",
            "(614, 33, 2)\n",
            "(416, 33, 2)\n",
            "(478, 33, 2)\n",
            "(489, 33, 2)\n",
            "(319, 33, 2)\n",
            "(299, 33, 2)\n",
            "(373, 33, 2)\n",
            "(278, 33, 2)\n",
            "(391, 33, 2)\n",
            "(459, 33, 2)\n",
            "(470, 33, 2)\n",
            "(478, 33, 2)\n",
            "(395, 33, 2)\n",
            "(493, 33, 2)\n",
            "(442, 33, 2)\n",
            "(523, 33, 2)\n",
            "(529, 33, 2)\n",
            "(514, 33, 2)\n",
            "(463, 33, 2)\n",
            "(311, 33, 2)\n",
            "(284, 33, 2)\n",
            "(364, 33, 2)\n",
            "(321, 33, 2)\n",
            "(469, 33, 2)\n",
            "(418, 33, 2)\n",
            "(484, 33, 2)\n",
            "(389, 33, 2)\n",
            "(404, 33, 2)\n",
            "(392, 33, 2)\n",
            "(449, 33, 2)\n",
            "(384, 33, 2)\n",
            "(509, 33, 2)\n",
            "(445, 33, 2)\n",
            "(549, 33, 2)\n",
            "(534, 33, 2)\n",
            "(474, 33, 2)\n",
            "(314, 33, 2)\n",
            "(514, 33, 2)\n",
            "(455, 33, 2)\n",
            "(671, 33, 2)\n",
            "(520, 33, 2)\n",
            "(698, 33, 2)\n",
            "(566, 33, 2)\n",
            "(500, 33, 2)\n",
            "(491, 33, 2)\n",
            "(335, 33, 2)\n",
            "(555, 33, 2)\n",
            "(544, 33, 2)\n",
            "(489, 33, 2)\n",
            "(329, 33, 2)\n",
            "(630, 33, 2)\n",
            "(580, 33, 2)\n",
            "(702, 33, 2)\n",
            "(597, 33, 2)\n",
            "(665, 33, 2)\n",
            "(491, 33, 2)\n",
            "(450, 33, 2)\n",
            "(507, 33, 2)\n",
            "(455, 33, 2)\n",
            "(429, 33, 2)\n",
            "(372, 33, 2)\n",
            "(513, 33, 2)\n",
            "(393, 33, 2)\n",
            "(579, 33, 2)\n",
            "(483, 33, 2)\n",
            "(551, 33, 2)\n",
            "(594, 33, 2)\n",
            "(243, 33, 2)\n",
            "(288, 33, 2)\n",
            "(215, 33, 2)\n",
            "(256, 33, 2)\n",
            "(227, 33, 2)\n",
            "(332, 33, 2)\n",
            "(222, 33, 2)\n",
            "(296, 33, 2)\n",
            "(212, 33, 2)\n",
            "(292, 33, 2)\n",
            "(222, 33, 2)\n",
            "(302, 33, 2)\n",
            "(263, 33, 2)\n",
            "(340, 33, 2)\n",
            "(298, 33, 2)\n",
            "(315, 33, 2)\n",
            "(262, 33, 2)\n",
            "(307, 33, 2)\n",
            "(215, 33, 2)\n",
            "(343, 33, 2)\n",
            "(260, 33, 2)\n",
            "(357, 33, 2)\n",
            "(230, 33, 2)\n",
            "(317, 33, 2)\n",
            "(242, 33, 2)\n",
            "(407, 33, 2)\n",
            "(227, 33, 2)\n",
            "(272, 33, 2)\n",
            "(282, 33, 2)\n",
            "(340, 33, 2)\n",
            "(237, 33, 2)\n",
            "(289, 33, 2)\n",
            "(148, 33, 2)\n",
            "(182, 33, 2)\n",
            "(129, 33, 2)\n",
            "(189, 33, 2)\n",
            "(127, 33, 2)\n",
            "(207, 33, 2)\n",
            "(154, 33, 2)\n",
            "(191, 33, 2)\n",
            "(187, 33, 2)\n",
            "(242, 33, 2)\n",
            "(192, 33, 2)\n",
            "(251, 33, 2)\n",
            "(216, 33, 2)\n",
            "(229, 33, 2)\n",
            "(180, 33, 2)\n",
            "(232, 33, 2)\n",
            "(192, 33, 2)\n",
            "(217, 33, 2)\n",
            "(176, 33, 2)\n",
            "(225, 33, 2)\n",
            "(211, 33, 2)\n",
            "(272, 33, 2)\n",
            "(201, 33, 2)\n",
            "(209, 33, 2)\n",
            "(142, 33, 2)\n",
            "(224, 33, 2)\n",
            "(147, 33, 2)\n",
            "(227, 33, 2)\n",
            "(204, 33, 2)\n",
            "(225, 33, 2)\n",
            "(172, 33, 2)\n",
            "(238, 33, 2)\n",
            "(402, 33, 2)\n",
            "(667, 33, 2)\n",
            "(402, 33, 2)\n",
            "(380, 33, 2)\n",
            "(352, 33, 2)\n",
            "(645, 33, 2)\n",
            "(322, 33, 2)\n",
            "(527, 33, 2)\n",
            "(397, 33, 2)\n",
            "(562, 33, 2)\n",
            "(397, 33, 2)\n",
            "(442, 33, 2)\n",
            "(497, 33, 2)\n",
            "(595, 33, 2)\n",
            "(410, 33, 2)\n",
            "(467, 33, 2)\n",
            "(459, 33, 2)\n",
            "(597, 33, 2)\n",
            "(345, 33, 2)\n",
            "(572, 33, 2)\n",
            "(406, 33, 2)\n",
            "(577, 33, 2)\n",
            "(422, 33, 2)\n",
            "(461, 33, 2)\n",
            "(433, 33, 2)\n",
            "(647, 33, 2)\n",
            "(385, 33, 2)\n",
            "(464, 33, 2)\n",
            "(377, 33, 2)\n",
            "(454, 33, 2)\n",
            "(364, 33, 2)\n",
            "(360, 33, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augmenting data"
      ],
      "metadata": {
        "id": "dfLnjXfuXlhc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3coGl7oeaI8Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "593d749e-5259-42f6-fe73-933647773a7a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-9cea8305faf0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_aug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_aug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-25de42abb573>\u001b[0m in \u001b[0;36mdata_augment\u001b[0;34m(x_df, y_df, size)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0my_rotated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_noised2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_flipped\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#rotate2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mx_rotate2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_flipped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'broadcast'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mx_rotated2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_rotated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_rotate2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0my_rotated2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_rotated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_flipped\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8846\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8847\u001b[0m         )\n\u001b[0;32m-> 8848\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"apply\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8850\u001b[0m     def applymap(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;31m# broadcasting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"broadcast\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0;31m# one axis empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_broadcast\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_broadcast\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m             \u001b[0mares\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-25de42abb573>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0my_rotated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_noised2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_flipped\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#rotate2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mx_rotate2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_flipped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'broadcast'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mx_rotated2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_rotated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_rotate2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0my_rotated2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_rotated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_flipped\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-03d6d0ba7b91>\u001b[0m in \u001b[0;36mrotate\u001b[0;34m(frame, p)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mrotated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrotate_joint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mjoint\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrotated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-03d6d0ba7b91>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mrotated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrotate_joint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mjoint\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrotated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-03d6d0ba7b91>\u001b[0m in \u001b[0;36mrotate_joint\u001b[0;34m(joint, p)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpsi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeg2rad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     a = np.array([[np.cos(psi), -np.sin(psi)],\n\u001b[0m\u001b[1;32m      6\u001b[0m                   [np.sin(psi), np.cos(psi)]])\n\u001b[1;32m      7\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "x_aug, y_aug = data_augment(x_1, y_1, 21120)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_arr1 =  df_train.to_numpy()\n",
        "y_train_arr1 = df_train_y.to_numpy()"
      ],
      "metadata": {
        "id": "ZOJr_VS8Xyx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "Rhkg5sGe3Po5",
        "outputId": "a020bb1e-3a45-4322-c5bc-24009996282d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       pos0_coord0  pos0_coord1  pos1_coord0  pos1_coord1  pos2_coord0  \\\n",
              "0         0.431053    -0.255754     0.429344    -0.237740     0.430456   \n",
              "1         0.430692    -0.261084     0.429278    -0.248600     0.430019   \n",
              "2         0.434268    -0.294032     0.433403    -0.278151     0.434350   \n",
              "3         0.455328    -0.307812     0.457614    -0.290123     0.459554   \n",
              "4         0.446774    -0.301506     0.448559    -0.284090     0.450157   \n",
              "...            ...          ...          ...          ...          ...   \n",
              "12251     0.503593    -0.169135     0.508932    -0.151420     0.510543   \n",
              "12252     0.307091    -0.161825     0.311640    -0.145861     0.313125   \n",
              "12253     0.135274    -0.152632     0.139560    -0.134648     0.141142   \n",
              "12254    -0.017047    -0.153857    -0.005760    -0.134571    -0.002282   \n",
              "12255    -0.039782    -0.105309    -0.024868    -0.095427    -0.020878   \n",
              "\n",
              "       pos2_coord1  pos3_coord0  pos3_coord1  pos4_coord0  pos4_coord1  ...  \\\n",
              "0        -0.236740     0.432377    -0.235841     0.429509    -0.238156  ...   \n",
              "1        -0.248790     0.431941    -0.248964     0.431956    -0.246020  ...   \n",
              "2        -0.276948     0.436262    -0.275621     0.435121    -0.277872  ...   \n",
              "3        -0.288513     0.461482    -0.286610     0.454851    -0.290862  ...   \n",
              "4        -0.283285     0.451860    -0.282345     0.448106    -0.283687  ...   \n",
              "...            ...          ...          ...          ...          ...  ...   \n",
              "12251    -0.150991     0.512283    -0.150644     0.508558    -0.151559  ...   \n",
              "12252    -0.145412     0.314747    -0.145094     0.311178    -0.146355  ...   \n",
              "12253    -0.134480     0.142857    -0.134525     0.139910    -0.134173  ...   \n",
              "12254    -0.133610     0.001242    -0.132702    -0.012986    -0.134570  ...   \n",
              "12255    -0.095557    -0.016876    -0.095750    -0.034295    -0.095863  ...   \n",
              "\n",
              "       pos28_coord0  pos28_coord1  pos29_coord0  pos29_coord1  pos30_coord0  \\\n",
              "0      4.865689e-01     -0.711372  5.376064e-01     -0.776412  4.991050e-01   \n",
              "1      5.216489e-01     -0.764044  5.374876e-01     -0.789167  5.429487e-01   \n",
              "2      5.182485e-01     -0.770929  5.386723e-01     -0.805068  5.378800e-01   \n",
              "3      5.167971e-01     -0.767876  5.474633e-01     -0.790923  5.304948e-01   \n",
              "4      5.146061e-01     -0.756731  5.387427e-01     -0.816001  5.265035e-01   \n",
              "...             ...           ...           ...           ...           ...   \n",
              "12251  4.739248e-01     -0.798173  5.919481e-01     -0.846770  4.791176e-01   \n",
              "12252  4.508335e-01     -0.763677  3.084679e-01     -0.827925  4.742016e-01   \n",
              "12253  1.075507e-01     -0.762820  2.561838e-01     -0.816282  1.105954e-01   \n",
              "12254  7.595568e-02     -0.731407  1.066303e-01     -0.755185  1.003642e-01   \n",
              "12255  1.447429e-17     -0.669719  1.643988e-17     -0.713988  1.428243e-17   \n",
              "\n",
              "       pos30_coord1  pos31_coord0  pos31_coord1  pos32_coord0  pos32_coord1  \n",
              "0         -0.697112  4.783478e-01     -0.800319  4.478542e-01     -0.733971  \n",
              "1         -0.788224  4.777910e-01     -0.800630  4.846908e-01     -0.809611  \n",
              "2         -0.796981  4.833162e-01     -0.804609  4.800399e-01     -0.807856  \n",
              "3         -0.789766  5.181044e-01     -0.816746  4.728185e-01     -0.807981  \n",
              "4         -0.761198  4.976739e-01     -0.814179  4.765528e-01     -0.786081  \n",
              "...             ...           ...           ...           ...           ...  \n",
              "12251     -0.823765  5.301022e-01     -0.854424  4.171441e-01     -0.827505  \n",
              "12252     -0.794110  2.383682e-01     -0.836208  4.381360e-01     -0.818604  \n",
              "12253     -0.786577  1.934075e-01     -0.829842  5.264505e-02     -0.798001  \n",
              "12254     -0.743144  5.168585e-02     -0.795539  5.702913e-02     -0.787313  \n",
              "12255     -0.688582  1.087827e-18     -0.776477  8.928844e-18     -0.737340  \n",
              "\n",
              "[12256 rows x 66 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-436486e3-e55f-4c41-9cb6-e77dd02d6c5c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pos0_coord0</th>\n",
              "      <th>pos0_coord1</th>\n",
              "      <th>pos1_coord0</th>\n",
              "      <th>pos1_coord1</th>\n",
              "      <th>pos2_coord0</th>\n",
              "      <th>pos2_coord1</th>\n",
              "      <th>pos3_coord0</th>\n",
              "      <th>pos3_coord1</th>\n",
              "      <th>pos4_coord0</th>\n",
              "      <th>pos4_coord1</th>\n",
              "      <th>...</th>\n",
              "      <th>pos28_coord0</th>\n",
              "      <th>pos28_coord1</th>\n",
              "      <th>pos29_coord0</th>\n",
              "      <th>pos29_coord1</th>\n",
              "      <th>pos30_coord0</th>\n",
              "      <th>pos30_coord1</th>\n",
              "      <th>pos31_coord0</th>\n",
              "      <th>pos31_coord1</th>\n",
              "      <th>pos32_coord0</th>\n",
              "      <th>pos32_coord1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.431053</td>\n",
              "      <td>-0.255754</td>\n",
              "      <td>0.429344</td>\n",
              "      <td>-0.237740</td>\n",
              "      <td>0.430456</td>\n",
              "      <td>-0.236740</td>\n",
              "      <td>0.432377</td>\n",
              "      <td>-0.235841</td>\n",
              "      <td>0.429509</td>\n",
              "      <td>-0.238156</td>\n",
              "      <td>...</td>\n",
              "      <td>4.865689e-01</td>\n",
              "      <td>-0.711372</td>\n",
              "      <td>5.376064e-01</td>\n",
              "      <td>-0.776412</td>\n",
              "      <td>4.991050e-01</td>\n",
              "      <td>-0.697112</td>\n",
              "      <td>4.783478e-01</td>\n",
              "      <td>-0.800319</td>\n",
              "      <td>4.478542e-01</td>\n",
              "      <td>-0.733971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.430692</td>\n",
              "      <td>-0.261084</td>\n",
              "      <td>0.429278</td>\n",
              "      <td>-0.248600</td>\n",
              "      <td>0.430019</td>\n",
              "      <td>-0.248790</td>\n",
              "      <td>0.431941</td>\n",
              "      <td>-0.248964</td>\n",
              "      <td>0.431956</td>\n",
              "      <td>-0.246020</td>\n",
              "      <td>...</td>\n",
              "      <td>5.216489e-01</td>\n",
              "      <td>-0.764044</td>\n",
              "      <td>5.374876e-01</td>\n",
              "      <td>-0.789167</td>\n",
              "      <td>5.429487e-01</td>\n",
              "      <td>-0.788224</td>\n",
              "      <td>4.777910e-01</td>\n",
              "      <td>-0.800630</td>\n",
              "      <td>4.846908e-01</td>\n",
              "      <td>-0.809611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.434268</td>\n",
              "      <td>-0.294032</td>\n",
              "      <td>0.433403</td>\n",
              "      <td>-0.278151</td>\n",
              "      <td>0.434350</td>\n",
              "      <td>-0.276948</td>\n",
              "      <td>0.436262</td>\n",
              "      <td>-0.275621</td>\n",
              "      <td>0.435121</td>\n",
              "      <td>-0.277872</td>\n",
              "      <td>...</td>\n",
              "      <td>5.182485e-01</td>\n",
              "      <td>-0.770929</td>\n",
              "      <td>5.386723e-01</td>\n",
              "      <td>-0.805068</td>\n",
              "      <td>5.378800e-01</td>\n",
              "      <td>-0.796981</td>\n",
              "      <td>4.833162e-01</td>\n",
              "      <td>-0.804609</td>\n",
              "      <td>4.800399e-01</td>\n",
              "      <td>-0.807856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.455328</td>\n",
              "      <td>-0.307812</td>\n",
              "      <td>0.457614</td>\n",
              "      <td>-0.290123</td>\n",
              "      <td>0.459554</td>\n",
              "      <td>-0.288513</td>\n",
              "      <td>0.461482</td>\n",
              "      <td>-0.286610</td>\n",
              "      <td>0.454851</td>\n",
              "      <td>-0.290862</td>\n",
              "      <td>...</td>\n",
              "      <td>5.167971e-01</td>\n",
              "      <td>-0.767876</td>\n",
              "      <td>5.474633e-01</td>\n",
              "      <td>-0.790923</td>\n",
              "      <td>5.304948e-01</td>\n",
              "      <td>-0.789766</td>\n",
              "      <td>5.181044e-01</td>\n",
              "      <td>-0.816746</td>\n",
              "      <td>4.728185e-01</td>\n",
              "      <td>-0.807981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.446774</td>\n",
              "      <td>-0.301506</td>\n",
              "      <td>0.448559</td>\n",
              "      <td>-0.284090</td>\n",
              "      <td>0.450157</td>\n",
              "      <td>-0.283285</td>\n",
              "      <td>0.451860</td>\n",
              "      <td>-0.282345</td>\n",
              "      <td>0.448106</td>\n",
              "      <td>-0.283687</td>\n",
              "      <td>...</td>\n",
              "      <td>5.146061e-01</td>\n",
              "      <td>-0.756731</td>\n",
              "      <td>5.387427e-01</td>\n",
              "      <td>-0.816001</td>\n",
              "      <td>5.265035e-01</td>\n",
              "      <td>-0.761198</td>\n",
              "      <td>4.976739e-01</td>\n",
              "      <td>-0.814179</td>\n",
              "      <td>4.765528e-01</td>\n",
              "      <td>-0.786081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12251</th>\n",
              "      <td>0.503593</td>\n",
              "      <td>-0.169135</td>\n",
              "      <td>0.508932</td>\n",
              "      <td>-0.151420</td>\n",
              "      <td>0.510543</td>\n",
              "      <td>-0.150991</td>\n",
              "      <td>0.512283</td>\n",
              "      <td>-0.150644</td>\n",
              "      <td>0.508558</td>\n",
              "      <td>-0.151559</td>\n",
              "      <td>...</td>\n",
              "      <td>4.739248e-01</td>\n",
              "      <td>-0.798173</td>\n",
              "      <td>5.919481e-01</td>\n",
              "      <td>-0.846770</td>\n",
              "      <td>4.791176e-01</td>\n",
              "      <td>-0.823765</td>\n",
              "      <td>5.301022e-01</td>\n",
              "      <td>-0.854424</td>\n",
              "      <td>4.171441e-01</td>\n",
              "      <td>-0.827505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12252</th>\n",
              "      <td>0.307091</td>\n",
              "      <td>-0.161825</td>\n",
              "      <td>0.311640</td>\n",
              "      <td>-0.145861</td>\n",
              "      <td>0.313125</td>\n",
              "      <td>-0.145412</td>\n",
              "      <td>0.314747</td>\n",
              "      <td>-0.145094</td>\n",
              "      <td>0.311178</td>\n",
              "      <td>-0.146355</td>\n",
              "      <td>...</td>\n",
              "      <td>4.508335e-01</td>\n",
              "      <td>-0.763677</td>\n",
              "      <td>3.084679e-01</td>\n",
              "      <td>-0.827925</td>\n",
              "      <td>4.742016e-01</td>\n",
              "      <td>-0.794110</td>\n",
              "      <td>2.383682e-01</td>\n",
              "      <td>-0.836208</td>\n",
              "      <td>4.381360e-01</td>\n",
              "      <td>-0.818604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12253</th>\n",
              "      <td>0.135274</td>\n",
              "      <td>-0.152632</td>\n",
              "      <td>0.139560</td>\n",
              "      <td>-0.134648</td>\n",
              "      <td>0.141142</td>\n",
              "      <td>-0.134480</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>-0.134525</td>\n",
              "      <td>0.139910</td>\n",
              "      <td>-0.134173</td>\n",
              "      <td>...</td>\n",
              "      <td>1.075507e-01</td>\n",
              "      <td>-0.762820</td>\n",
              "      <td>2.561838e-01</td>\n",
              "      <td>-0.816282</td>\n",
              "      <td>1.105954e-01</td>\n",
              "      <td>-0.786577</td>\n",
              "      <td>1.934075e-01</td>\n",
              "      <td>-0.829842</td>\n",
              "      <td>5.264505e-02</td>\n",
              "      <td>-0.798001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12254</th>\n",
              "      <td>-0.017047</td>\n",
              "      <td>-0.153857</td>\n",
              "      <td>-0.005760</td>\n",
              "      <td>-0.134571</td>\n",
              "      <td>-0.002282</td>\n",
              "      <td>-0.133610</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>-0.132702</td>\n",
              "      <td>-0.012986</td>\n",
              "      <td>-0.134570</td>\n",
              "      <td>...</td>\n",
              "      <td>7.595568e-02</td>\n",
              "      <td>-0.731407</td>\n",
              "      <td>1.066303e-01</td>\n",
              "      <td>-0.755185</td>\n",
              "      <td>1.003642e-01</td>\n",
              "      <td>-0.743144</td>\n",
              "      <td>5.168585e-02</td>\n",
              "      <td>-0.795539</td>\n",
              "      <td>5.702913e-02</td>\n",
              "      <td>-0.787313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12255</th>\n",
              "      <td>-0.039782</td>\n",
              "      <td>-0.105309</td>\n",
              "      <td>-0.024868</td>\n",
              "      <td>-0.095427</td>\n",
              "      <td>-0.020878</td>\n",
              "      <td>-0.095557</td>\n",
              "      <td>-0.016876</td>\n",
              "      <td>-0.095750</td>\n",
              "      <td>-0.034295</td>\n",
              "      <td>-0.095863</td>\n",
              "      <td>...</td>\n",
              "      <td>1.447429e-17</td>\n",
              "      <td>-0.669719</td>\n",
              "      <td>1.643988e-17</td>\n",
              "      <td>-0.713988</td>\n",
              "      <td>1.428243e-17</td>\n",
              "      <td>-0.688582</td>\n",
              "      <td>1.087827e-18</td>\n",
              "      <td>-0.776477</td>\n",
              "      <td>8.928844e-18</td>\n",
              "      <td>-0.737340</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12256 rows × 66 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-436486e3-e55f-4c41-9cb6-e77dd02d6c5c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-436486e3-e55f-4c41-9cb6-e77dd02d6c5c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-436486e3-e55f-4c41-9cb6-e77dd02d6c5c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Action labels are repeated for each frame in order to generate features correctly"
      ],
      "metadata": {
        "id": "lB-CCCVXX0lZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqospqPDaI8Z"
      },
      "outputs": [],
      "source": [
        "y_train1 = []\n",
        "video_indices1 = []\n",
        "for i in range(len(y_train_arr1)):\n",
        "    y_train1.extend([y_train_arr1[i][0]]*32)\n",
        "    video_indices1.extend([i]*32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlQ7-kkuaI8a",
        "outputId": "648673a0-627e-4307-97d2-d422ddf6da2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train_arr (12256, 66)\n",
            "y_train_arr (383, 1)\n"
          ]
        }
      ],
      "source": [
        "print('x_train_arr', x_train_arr1.shape)\n",
        "print('y_train_arr', y_train_arr1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ISwc5ehaI8b"
      },
      "outputs": [],
      "source": [
        "x_test_arr1 =  df_test.to_numpy()\n",
        "y_test_arr1 = df_test_y.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dt6mVhxmaI8c"
      },
      "outputs": [],
      "source": [
        "y_test1 = []\n",
        "video_indices_test1 = []\n",
        "for i in range(len(y_test_arr1)):\n",
        "    y_test1.extend([y_test_arr1[i][0]]*32)\n",
        "    video_indices_test1.extend([i]*32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_test_arr1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "strjVCsw6UrL",
        "outputId": "19bde398-7ba6-4aec-d01c-3a8f4745cbda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6912"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting features for train set"
      ],
      "metadata": {
        "id": "N7tDYK1_YHgq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tusUcgD-aI8c",
        "outputId": "aa567d53-1316-4f83-c13d-e3e276f307d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting time-serials features ...\n",
            "0/12256, 1000/12256, 2000/12256, 3000/12256, 4000/12256, 5000/12256, 6000/12256, 7000/12256, 8000/12256, 9000/12256, 10000/12256, 11000/12256, 12000/12256, \n",
            "X.shape = (10724, 674), len(Y) = 10724\n"
          ]
        }
      ],
      "source": [
        "# Process features\n",
        "print(\"\\nExtracting time-serials features ...\")\n",
        "X_train1, y_tr, X_train_d1= process_features(x_train_arr1, y_train1, video_indices1, CLASSES)\n",
        "print(f\"X.shape = {X_train1.shape}, len(Y) = {len(y_tr)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting features for test set"
      ],
      "metadata": {
        "id": "-1thl0CtYLrB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBZxNqGGaI8d",
        "outputId": "020a3f69-dded-4ac9-ac1b-c25849768aba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting time-serials features ...\n",
            "0/6912, 1000/6912, 2000/6912, 3000/6912, 4000/6912, 5000/6912, 6000/6912, \n",
            "X.shape = (6048, 674), len(Y) = 6048\n",
            "TOTAL TIME:  3.7994000911712646\n"
          ]
        }
      ],
      "source": [
        "# Process features\n",
        "print(\"\\nExtracting time-serials features ...\")\n",
        "time_start = time.time()\n",
        "X_test1, y_te, X_test_d1 = process_features(x_test_arr1, y_test1, video_indices_test1, CLASSES)\n",
        "time_stop = time.time()\n",
        "print(f\"X.shape = {X_test1.shape}, len(Y) = {len(y_te)}\")\n",
        "print(\"TOTAL TIME:  {}\".format(time_stop - time_start))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_tr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Wky1-1La_9S",
        "outputId": "213be1fa-8050-451d-a9a5-832932aa3d59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 5, 5, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Jt7hm31aI8e"
      },
      "source": [
        "## PCA for joint based features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Joint based features reduced to 100 dimension"
      ],
      "metadata": {
        "id": "ZjAwFE61YXvP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi-PDASdaI8e",
        "outputId": "dd2193b2-1a4f-4c3e-8fb0-663462ed33ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum eig values: 0.9994232841951038\n",
            "After PCA, X.shape =  (10724, 100)\n"
          ]
        }
      ],
      "source": [
        "# -- PCA\n",
        "NUM_FEATURES_FROM_PCA = 100\n",
        "\n",
        "n_components = min(NUM_FEATURES_FROM_PCA, X_train1.shape[1])\n",
        "pca = PCA(n_components=n_components, whiten=True)\n",
        "pca.fit(X_train1)\n",
        "# print(\"Sum eig values:\", np.sum(self.pca.singular_values_))\n",
        "print(\"Sum eig values:\", np.sum(pca.explained_variance_ratio_))\n",
        "X_tr1 = pca.transform(X_train1)\n",
        "print(\"After PCA, X.shape = \", X_tr1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr1 = X_train1"
      ],
      "metadata": {
        "id": "zPoCwFP7XOer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FQ9wO8N_XRLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y1Cp6J9aI8f",
        "outputId": "f7c09ed0-8a3d-4a9d-92f8-0be73ea4c672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOTAL TIME:  0.05937600135803223\n"
          ]
        }
      ],
      "source": [
        "time_start = time.time()\n",
        "X_te1 = pca.transform(X_test1)\n",
        "time_stop = time.time()\n",
        "print(\"TOTAL TIME:  {}\".format(time_stop - time_start))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_te1 = X_test1"
      ],
      "metadata": {
        "id": "8sC_VLyAXRtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXBsmqUDaI8f"
      },
      "source": [
        "## PCA for distance feature"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distance feature dimension reduced to 50"
      ],
      "metadata": {
        "id": "pB1QMYeoYrho"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8bQNIcGaI8f",
        "outputId": "acf23ac9-76bd-4055-f711-3ad1e8ed6903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum eig values: 0.9963187796485521\n",
            "After PCA, X.shape =  (10724, 50)\n"
          ]
        }
      ],
      "source": [
        "# -- PCA\n",
        "NUM_FEATURES_FROM_PCA = 50\n",
        "\n",
        "n_components = min(NUM_FEATURES_FROM_PCA, X_train_d1.shape[1])\n",
        "pca = PCA(n_components=n_components, whiten=True)\n",
        "pca.fit(X_train_d1)\n",
        "# print(\"Sum eig values:\", np.sum(self.pca.singular_values_))\n",
        "print(\"Sum eig values:\", np.sum(pca.explained_variance_ratio_))\n",
        "X_tr_d1 = pca.transform(X_train_d1)\n",
        "print(\"After PCA, X.shape = \", X_tr_d1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_tr_d1 = X_train_d1"
      ],
      "metadata": {
        "id": "EMkAtz9oXU-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzYXTTORaI8g",
        "outputId": "c7f1df54-213a-4014-f02d-6aac2ecf87b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOTAL TIME:  0.12411046028137207\n"
          ]
        }
      ],
      "source": [
        "time_start = time.time()\n",
        "X_te_d1 = pca.transform(X_test_d1)\n",
        "time_stop = time.time()\n",
        "print(\"TOTAL TIME:  {}\".format(time_stop - time_start))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_te_d1 = X_test_d1"
      ],
      "metadata": {
        "id": "9m-cHip8XagT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concatenate Joint based and Distance based features"
      ],
      "metadata": {
        "id": "rkWVtt_KYzRq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLwJHx74aI8j",
        "outputId": "c9072c5a-731c-4c3e-87be-2a6a7002bffa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10724, 150)"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ],
      "source": [
        "X_train1 = np.concatenate((X_tr1, X_tr_d1), axis=1)\n",
        "X_train1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOKz041IaI8k",
        "outputId": "33ccbd7c-a994-4dfa-dade-15ba185ef159"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6048, 150)"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ],
      "source": [
        "X_test1 = np.concatenate((X_te1, X_te_d1), axis=1)\n",
        "X_test1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Sy18qmcaI8k",
        "outputId": "670d4b2e-4dcb-4d46-e0f7-08eb80da74d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(383, 28, 150)"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ],
      "source": [
        "blocks = int(len(X_train1) / 28)\n",
        "X_train1 = np.array(np.split(X_train1, blocks))\n",
        "X_train1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyScg7MnaI8l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7159e556-a0b5-4e93-8fed-3f160c2cb478"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(216, 28, 150)"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ],
      "source": [
        "blocks = int(len(X_test1) / 28)\n",
        "X_test1 = np.array(np.split(X_test1, blocks))\n",
        "X_test1.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWtFXCr9aI8l"
      },
      "source": [
        "# Split 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acG0lq4SaI8l"
      },
      "outputs": [],
      "source": [
        "x_2 = pd.read_csv(path+'jhmdb/data/GT_train_2.csv', header=None)\n",
        "y_2 = pd.read_csv(path+'jhmdb/data/GT_train_y_2.csv', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cqfvaw7AaI8l",
        "outputId": "0bcc5bd3-ffd0-42c6-d2c1-8344e520e92d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21056, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "x_2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUfQ3UCaaI8m"
      },
      "outputs": [],
      "source": [
        "x_2[x_2.columns[1::2]] = x_2[x_2.columns[1::2]].apply(lambda y: -y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQE1x4OFaI8m"
      },
      "outputs": [],
      "source": [
        "x_aug2, y_aug2 = data_augment(x_2, y_2, 21056)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7xzxoZhaI8n"
      },
      "outputs": [],
      "source": [
        "x_train_arr2 =  x_aug2.to_numpy()\n",
        "y_train_arr2 = y_aug2.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztU4LDHjaI8s"
      },
      "outputs": [],
      "source": [
        "y_train2 = []\n",
        "video_indices2 = []\n",
        "for i in range(len(y_train_arr2)):\n",
        "    y_train2.extend([y_train_arr2[i][0]]*32)\n",
        "    video_indices2.extend([i]*32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yynTLx7zaI8t",
        "outputId": "53e21b4d-a60c-46ca-b3fb-c0c9cab95c41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_test_arr (252672, 30)\n",
            "y_test_arr (7896, 1)\n"
          ]
        }
      ],
      "source": [
        "print('x_test_arr', x_train_arr2.shape)\n",
        "print('y_test_arr', y_train_arr2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5PLfUU_aI8u"
      },
      "outputs": [],
      "source": [
        "x_2_test = pd.read_csv(path+'jhmdb/data/GT_test_2.csv', header=None)\n",
        "y_2_test = pd.read_csv(path+'jhmdb/data/GT_test_y_2.csv', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LT3IZtFtaI8u"
      },
      "outputs": [],
      "source": [
        "x_2_test[x_2_test.columns[1::2]] = x_2_test[x_2_test.columns[1::2]].apply(lambda y: -y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIcvvqJqaI83"
      },
      "outputs": [],
      "source": [
        "x_test_arr2 =  x_2_test.to_numpy()\n",
        "y_test_arr2 = y_2_test.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRaYXdkIaI88"
      },
      "outputs": [],
      "source": [
        "y_test2 = []\n",
        "video_indices_test2 = []\n",
        "for i in range(len(y_test_arr2)):\n",
        "    y_test2.extend([y_test_arr2[i][0]]*32)\n",
        "    video_indices_test2.extend([i]*32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGbddjdDaI88",
        "outputId": "671d92d3-a4a6-482f-bd5a-ac7dac90eb69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting time-serials features ...\n",
            "0/252672, 1000/252672, 2000/252672, 3000/252672, 4000/252672, 5000/252672, 6000/252672, 7000/252672, 8000/252672, 9000/252672, 10000/252672, 11000/252672, 12000/252672, 13000/252672, 14000/252672, 15000/252672, 16000/252672, 17000/252672, 18000/252672, 19000/252672, 20000/252672, 21000/252672, 22000/252672, 23000/252672, 24000/252672, 25000/252672, 26000/252672, 27000/252672, 28000/252672, 29000/252672, 30000/252672, 31000/252672, 32000/252672, 33000/252672, 34000/252672, 35000/252672, 36000/252672, 37000/252672, 38000/252672, 39000/252672, 40000/252672, 41000/252672, 42000/252672, 43000/252672, 44000/252672, 45000/252672, 46000/252672, 47000/252672, 48000/252672, 49000/252672, 50000/252672, 51000/252672, 52000/252672, 53000/252672, 54000/252672, 55000/252672, 56000/252672, 57000/252672, 58000/252672, 59000/252672, 60000/252672, 61000/252672, 62000/252672, 63000/252672, 64000/252672, 65000/252672, 66000/252672, 67000/252672, 68000/252672, 69000/252672, 70000/252672, 71000/252672, 72000/252672, 73000/252672, 74000/252672, 75000/252672, 76000/252672, 77000/252672, 78000/252672, 79000/252672, 80000/252672, 81000/252672, 82000/252672, 83000/252672, 84000/252672, 85000/252672, 86000/252672, 87000/252672, 88000/252672, 89000/252672, 90000/252672, 91000/252672, 92000/252672, 93000/252672, 94000/252672, 95000/252672, 96000/252672, 97000/252672, 98000/252672, 99000/252672, 100000/252672, 101000/252672, 102000/252672, 103000/252672, 104000/252672, 105000/252672, 106000/252672, 107000/252672, 108000/252672, 109000/252672, 110000/252672, 111000/252672, 112000/252672, 113000/252672, 114000/252672, 115000/252672, 116000/252672, 117000/252672, 118000/252672, 119000/252672, 120000/252672, 121000/252672, 122000/252672, 123000/252672, 124000/252672, 125000/252672, 126000/252672, 127000/252672, 128000/252672, 129000/252672, 130000/252672, 131000/252672, 132000/252672, 133000/252672, 134000/252672, 135000/252672, 136000/252672, 137000/252672, 138000/252672, 139000/252672, 140000/252672, 141000/252672, 142000/252672, 143000/252672, 144000/252672, 145000/252672, 146000/252672, 147000/252672, 148000/252672, 149000/252672, 150000/252672, 151000/252672, 152000/252672, 153000/252672, 154000/252672, 155000/252672, 156000/252672, 157000/252672, 158000/252672, 159000/252672, 160000/252672, 161000/252672, 162000/252672, 163000/252672, 164000/252672, 165000/252672, 166000/252672, 167000/252672, 168000/252672, 169000/252672, 170000/252672, 171000/252672, 172000/252672, 173000/252672, 174000/252672, 175000/252672, 176000/252672, 177000/252672, 178000/252672, 179000/252672, 180000/252672, 181000/252672, 182000/252672, 183000/252672, 184000/252672, 185000/252672, 186000/252672, 187000/252672, 188000/252672, 189000/252672, 190000/252672, 191000/252672, 192000/252672, 193000/252672, 194000/252672, 195000/252672, 196000/252672, 197000/252672, 198000/252672, 199000/252672, 200000/252672, 201000/252672, 202000/252672, 203000/252672, 204000/252672, 205000/252672, 206000/252672, 207000/252672, 208000/252672, 209000/252672, 210000/252672, 211000/252672, 212000/252672, 213000/252672, 214000/252672, 215000/252672, 216000/252672, 217000/252672, 218000/252672, 219000/252672, 220000/252672, 221000/252672, 222000/252672, 223000/252672, 224000/252672, 225000/252672, 226000/252672, 227000/252672, 228000/252672, 229000/252672, 230000/252672, 231000/252672, 232000/252672, 233000/252672, 234000/252672, 235000/252672, 236000/252672, 237000/252672, 238000/252672, 239000/252672, 240000/252672, 241000/252672, 242000/252672, 243000/252672, 244000/252672, 245000/252672, 246000/252672, 247000/252672, 248000/252672, 249000/252672, 250000/252672, 251000/252672, 252000/252672, \n",
            "X.shape = (221088, 350), len(Y) = 221088\n"
          ]
        }
      ],
      "source": [
        "# Process features\n",
        "print(\"\\nExtracting time-serials features ...\")\n",
        "X_train2, y_tr, X_train_d2= process_features(x_train_arr2, y_train2, video_indices2, CLASSES)\n",
        "print(f\"X.shape = {X_train2.shape}, len(Y) = {len(y_tr)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PX19k89aI89",
        "outputId": "2da9c92b-2b33-440c-847d-fff1438a2fbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting time-serials features ...\n",
            "0/8640, 1000/8640, 2000/8640, 3000/8640, 4000/8640, 5000/8640, 6000/8640, 7000/8640, 8000/8640, \n",
            "X.shape = (7560, 350), len(Y) = 7560\n",
            "TOTAL TIME:  1.8711984157562256\n"
          ]
        }
      ],
      "source": [
        "# Process features\n",
        "print(\"\\nExtracting time-serials features ...\")\n",
        "time_start = time.time()\n",
        "X_test2, y_te, X_test_d2 = process_features(x_test_arr2, y_test2, video_indices_test2, CLASSES)\n",
        "time_stop = time.time()\n",
        "print(f\"X.shape = {X_test2.shape}, len(Y) = {len(y_te)}\")\n",
        "print(\"TOTAL TIME:  {}\".format(time_stop - time_start))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XIgS1tWaI89",
        "outputId": "0d8d259f-a174-4f7f-c7d8-11b50275bbe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum eig values: 0.9906907859009909\n",
            "After PCA, X.shape =  (221088, 100)\n"
          ]
        }
      ],
      "source": [
        "# -- PCA\n",
        "NUM_FEATURES_FROM_PCA = 100\n",
        "\n",
        "n_components = min(NUM_FEATURES_FROM_PCA, X_train2.shape[1])\n",
        "pca = PCA(n_components=n_components, whiten=True)\n",
        "pca.fit(X_train2)\n",
        "# print(\"Sum eig values:\", np.sum(self.pca.singular_values_))\n",
        "print(\"Sum eig values:\", np.sum(pca.explained_variance_ratio_))\n",
        "X_tr2 = pca.transform(X_train2)\n",
        "print(\"After PCA, X.shape = \", X_tr2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzzmSLnVaI8-",
        "outputId": "4bf40f01-a9b7-45fb-ecb6-b271733df218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOTAL TIME:  0.022981643676757812\n"
          ]
        }
      ],
      "source": [
        "time_start = time.time()\n",
        "X_te2 = pca.transform(X_test2)\n",
        "time_stop = time.time()\n",
        "print(\"TOTAL TIME:  {}\".format(time_stop - time_start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbXgvgeYaI8_",
        "outputId": "30dd22b1-e814-4567-de78-8d1f81b8ad20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum eig values: 0.959312658419985\n",
            "After PCA, X.shape =  (221088, 50)\n"
          ]
        }
      ],
      "source": [
        "# -- PCA\n",
        "NUM_FEATURES_FROM_PCA = 50\n",
        "\n",
        "n_components = min(NUM_FEATURES_FROM_PCA, X_train_d2.shape[1])\n",
        "pca = PCA(n_components=n_components, whiten=True)\n",
        "pca.fit(X_train_d2)\n",
        "# print(\"Sum eig values:\", np.sum(self.pca.singular_values_))\n",
        "print(\"Sum eig values:\", np.sum(pca.explained_variance_ratio_))\n",
        "X_tr_d2 = pca.transform(X_train_d2)\n",
        "print(\"After PCA, X.shape = \", X_tr_d2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcB6j8cUaI8_",
        "outputId": "83b00318-930b-47df-c155-8b2fd27fbcc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOTAL TIME:  0.024183034896850586\n"
          ]
        }
      ],
      "source": [
        "time_start = time.time()\n",
        "X_te_d2 = pca.transform(X_test_d2)\n",
        "time_stop = time.time()\n",
        "print(\"TOTAL TIME:  {}\".format(time_stop - time_start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN1QtbxKaI9A",
        "outputId": "c3963f10-cd6c-4ae1-b0d0-9f75dda00fdb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7560, 150)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "X_train2 = np.concatenate((X_tr2, X_tr_d2), axis=1)\n",
        "X_test2 = np.concatenate((X_te2, X_te_d2), axis=1)\n",
        "X_test2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9mhoxT7aI9B",
        "outputId": "edbd4240-ba79-4141-c884-00a71a984884"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7896, 28, 150)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "blocks = int(len(X_train2) / 28)\n",
        "X_train2 = np.array(np.split(X_train2, blocks))\n",
        "\n",
        "blocks = int(len(X_test2) / 28)\n",
        "X_test2 = np.array(np.split(X_test2, blocks))\n",
        "\n",
        "X_train2.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w8R_mutaI9B"
      },
      "source": [
        "# Split 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XESDQvDaaI9B"
      },
      "outputs": [],
      "source": [
        "x_3 = pd.read_csv(path+'jhmdb/data/GT_train_3.csv', header=None)\n",
        "y_3 = pd.read_csv(path+'jhmdb/data/GT_train_y_3.csv', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPRorl6laI9C",
        "outputId": "e46541cc-177b-4bb2-fdf8-1a6ef0260318"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21216, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "x_3.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vd5A2zTWaI9C"
      },
      "outputs": [],
      "source": [
        "x_3[x_3.columns[1::2]] = x_3[x_3.columns[1::2]].apply(lambda y: -y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkxJ8VC2aI9D"
      },
      "outputs": [],
      "source": [
        "x_aug3, y_aug3 = data_augment(x_3, y_3, 21216)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdzOyWfuaI9D"
      },
      "outputs": [],
      "source": [
        "x_train_arr3 =  x_aug3.to_numpy()\n",
        "y_train_arr3 = y_aug3.to_numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGgqwPpQaI9D"
      },
      "outputs": [],
      "source": [
        "y_train3 = []\n",
        "video_indices3 = []\n",
        "for i in range(len(y_train_arr3)):\n",
        "    y_train3.extend([y_train_arr3[i][0]]*32)\n",
        "    video_indices3.extend([i]*32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBjxZkbFaI9E",
        "outputId": "a3526ab6-8ec9-41c6-fb79-af41605ff4ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_test_arr (254592, 30)\n",
            "y_test_arr (7956, 1)\n"
          ]
        }
      ],
      "source": [
        "print('x_test_arr', x_train_arr3.shape)\n",
        "print('y_test_arr', y_train_arr3.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wT840yIDaI9E"
      },
      "outputs": [],
      "source": [
        "x_3_test = pd.read_csv(path+'jhmdb/data/GT_test_3.csv', header=None)\n",
        "y_3_test = pd.read_csv(path+'jhmdb/data/GT_test_y_3.csv', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aYIkTlZaI9E"
      },
      "outputs": [],
      "source": [
        "x_3_test[x_3_test.columns[1::2]] = x_3_test[x_3_test.columns[1::2]].apply(lambda y: -y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJADQOxaaI9F"
      },
      "outputs": [],
      "source": [
        "x_test_arr3 =  x_3_test.to_numpy()\n",
        "y_test_arr3 = y_3_test.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5J-J5fpaI9F"
      },
      "outputs": [],
      "source": [
        "y_test3 = []\n",
        "video_indices_test3 = []\n",
        "for i in range(len(y_test_arr3)):\n",
        "    y_test3.extend([y_test_arr3[i][0]]*32)\n",
        "    video_indices_test3.extend([i]*32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKvNByD7aI9F",
        "outputId": "8fabc95d-0175-4583-dfa6-0b26734a8bea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting time-serials features ...\n",
            "0/254592, 1000/254592, 2000/254592, 3000/254592, 4000/254592, 5000/254592, 6000/254592, 7000/254592, 8000/254592, 9000/254592, 10000/254592, 11000/254592, 12000/254592, 13000/254592, 14000/254592, 15000/254592, 16000/254592, 17000/254592, 18000/254592, 19000/254592, 20000/254592, 21000/254592, 22000/254592, 23000/254592, 24000/254592, 25000/254592, 26000/254592, 27000/254592, 28000/254592, 29000/254592, 30000/254592, 31000/254592, 32000/254592, 33000/254592, 34000/254592, 35000/254592, 36000/254592, 37000/254592, 38000/254592, 39000/254592, 40000/254592, 41000/254592, 42000/254592, 43000/254592, 44000/254592, 45000/254592, 46000/254592, 47000/254592, 48000/254592, 49000/254592, 50000/254592, 51000/254592, 52000/254592, 53000/254592, 54000/254592, 55000/254592, 56000/254592, 57000/254592, 58000/254592, 59000/254592, 60000/254592, 61000/254592, 62000/254592, 63000/254592, 64000/254592, 65000/254592, 66000/254592, 67000/254592, 68000/254592, 69000/254592, 70000/254592, 71000/254592, 72000/254592, 73000/254592, 74000/254592, 75000/254592, 76000/254592, 77000/254592, 78000/254592, 79000/254592, 80000/254592, 81000/254592, 82000/254592, 83000/254592, 84000/254592, 85000/254592, 86000/254592, 87000/254592, 88000/254592, 89000/254592, 90000/254592, 91000/254592, 92000/254592, 93000/254592, 94000/254592, 95000/254592, 96000/254592, 97000/254592, 98000/254592, 99000/254592, 100000/254592, 101000/254592, 102000/254592, 103000/254592, 104000/254592, 105000/254592, 106000/254592, 107000/254592, 108000/254592, 109000/254592, 110000/254592, 111000/254592, 112000/254592, 113000/254592, 114000/254592, 115000/254592, 116000/254592, 117000/254592, 118000/254592, 119000/254592, 120000/254592, 121000/254592, 122000/254592, 123000/254592, 124000/254592, 125000/254592, 126000/254592, 127000/254592, 128000/254592, 129000/254592, 130000/254592, 131000/254592, 132000/254592, 133000/254592, 134000/254592, 135000/254592, 136000/254592, 137000/254592, 138000/254592, 139000/254592, 140000/254592, 141000/254592, 142000/254592, 143000/254592, 144000/254592, 145000/254592, 146000/254592, 147000/254592, 148000/254592, 149000/254592, 150000/254592, 151000/254592, 152000/254592, 153000/254592, 154000/254592, 155000/254592, 156000/254592, 157000/254592, 158000/254592, 159000/254592, 160000/254592, 161000/254592, 162000/254592, 163000/254592, 164000/254592, 165000/254592, 166000/254592, 167000/254592, 168000/254592, 169000/254592, 170000/254592, 171000/254592, 172000/254592, 173000/254592, 174000/254592, 175000/254592, 176000/254592, 177000/254592, 178000/254592, 179000/254592, 180000/254592, 181000/254592, 182000/254592, 183000/254592, 184000/254592, 185000/254592, 186000/254592, 187000/254592, 188000/254592, 189000/254592, 190000/254592, 191000/254592, 192000/254592, 193000/254592, 194000/254592, 195000/254592, 196000/254592, 197000/254592, 198000/254592, 199000/254592, 200000/254592, 201000/254592, 202000/254592, 203000/254592, 204000/254592, 205000/254592, 206000/254592, 207000/254592, 208000/254592, 209000/254592, 210000/254592, 211000/254592, 212000/254592, 213000/254592, 214000/254592, 215000/254592, 216000/254592, 217000/254592, 218000/254592, 219000/254592, 220000/254592, 221000/254592, 222000/254592, 223000/254592, 224000/254592, 225000/254592, 226000/254592, 227000/254592, 228000/254592, 229000/254592, 230000/254592, 231000/254592, 232000/254592, 233000/254592, 234000/254592, 235000/254592, 236000/254592, 237000/254592, 238000/254592, 239000/254592, 240000/254592, 241000/254592, 242000/254592, 243000/254592, 244000/254592, 245000/254592, 246000/254592, 247000/254592, 248000/254592, 249000/254592, 250000/254592, 251000/254592, 252000/254592, 253000/254592, 254000/254592, \n",
            "X.shape = (222768, 350), len(Y) = 222768\n"
          ]
        }
      ],
      "source": [
        "# Process features\n",
        "print(\"\\nExtracting time-serials features ...\")\n",
        "X_train3, y_tr, X_train_d3= process_features(x_train_arr3, y_train3, video_indices3, CLASSES)\n",
        "print(f\"X.shape = {X_train3.shape}, len(Y) = {len(y_tr)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqvfbSM2aI9F",
        "outputId": "0bf8386f-2365-4292-d715-0a977c2c923e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting time-serials features ...\n",
            "0/8480, 1000/8480, 2000/8480, 3000/8480, 4000/8480, 5000/8480, 6000/8480, 7000/8480, 8000/8480, \n",
            "X.shape = (7420, 350), len(Y) = 7420\n",
            "TOTAL TIME:  1.8547394275665283\n"
          ]
        }
      ],
      "source": [
        "# Process features\n",
        "print(\"\\nExtracting time-serials features ...\")\n",
        "time_start = time.time()\n",
        "X_test3, y_te, X_test_d3 = process_features(x_test_arr3, y_test3, video_indices_test3, CLASSES)\n",
        "time_stop = time.time()\n",
        "print(f\"X.shape = {X_test3.shape}, len(Y) = {len(y_te)}\")\n",
        "print(\"TOTAL TIME:  {}\".format(time_stop - time_start))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LxpoiK1aI9G",
        "outputId": "593aa0ab-6511-487a-c906-dc2f027c7f40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum eig values: 0.9908513744966687\n",
            "After PCA, X.shape =  (222768, 100)\n"
          ]
        }
      ],
      "source": [
        "# -- PCA\n",
        "NUM_FEATURES_FROM_PCA = 100\n",
        "\n",
        "n_components = min(NUM_FEATURES_FROM_PCA, X_train3.shape[1])\n",
        "pca = PCA(n_components=n_components, whiten=True)\n",
        "pca.fit(X_train3)\n",
        "# print(\"Sum eig values:\", np.sum(self.pca.singular_values_))\n",
        "print(\"Sum eig values:\", np.sum(pca.explained_variance_ratio_))\n",
        "X_tr3 = pca.transform(X_train3)\n",
        "print(\"After PCA, X.shape = \", X_tr3.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0s1T3JUaI9G",
        "outputId": "05d09f58-6481-4043-b1d9-f17bcecc4999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOTAL TIME:  0.02756977081298828\n"
          ]
        }
      ],
      "source": [
        "time_start = time.time()\n",
        "X_te3 = pca.transform(X_test3)\n",
        "time_stop = time.time()\n",
        "print(\"TOTAL TIME:  {}\".format(time_stop - time_start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFH374ncaI9G",
        "outputId": "f8d6d56a-5b24-486d-b94f-47a052fd9827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum eig values: 0.9605640243310695\n",
            "After PCA, X.shape =  (222768, 50)\n"
          ]
        }
      ],
      "source": [
        "# -- PCA\n",
        "NUM_FEATURES_FROM_PCA = 50\n",
        "\n",
        "n_components = min(NUM_FEATURES_FROM_PCA, X_train_d3.shape[1])\n",
        "pca = PCA(n_components=n_components, whiten=True)\n",
        "pca.fit(X_train_d3)\n",
        "# print(\"Sum eig values:\", np.sum(self.pca.singular_values_))\n",
        "print(\"Sum eig values:\", np.sum(pca.explained_variance_ratio_))\n",
        "X_tr_d3 = pca.transform(X_train_d3)\n",
        "print(\"After PCA, X.shape = \", X_tr_d3.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUHXaWGpaI9H",
        "outputId": "82d13907-a853-4efc-d4e0-47ebe0b51b73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOTAL TIME:  0.023772001266479492\n"
          ]
        }
      ],
      "source": [
        "time_start = time.time()\n",
        "X_te_d3 = pca.transform(X_test_d3)\n",
        "time_stop = time.time()\n",
        "print(\"TOTAL TIME:  {}\".format(time_stop - time_start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPXEBFtZaI9H",
        "outputId": "f1969683-56b1-4f92-f145-9fc59c5786de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7420, 150)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "X_train3 = np.concatenate((X_tr3, X_tr_d3), axis=1)\n",
        "X_test3 = np.concatenate((X_te3, X_te_d3), axis=1)\n",
        "X_test3.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPI32j0raI9H",
        "outputId": "89167a4f-b3f1-43b1-fc6f-349a6582d6a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7956, 28, 150)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "blocks = int(len(X_train3) / 28)\n",
        "X_train3 = np.array(np.split(X_train3, blocks))\n",
        "\n",
        "blocks = int(len(X_test3) / 28)\n",
        "X_test3 = np.array(np.split(X_test3, blocks))\n",
        "\n",
        "X_train3.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNkBpM9daI9J"
      },
      "source": [
        "# Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8SuTHxyaI9K"
      },
      "outputs": [],
      "source": [
        "MAX_SEQ_LENGTH = 28\n",
        "NUM_FEATURES = 150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "kkNlWnfzaI9K",
        "outputId": "2c6bfae5-fde3-49b4-a5a1-81bf9a2fe933"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(X shape, y shape, every X's mean, every X's standard deviation)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-053091a12126>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"(X shape, y shape, every X's mean, every X's standard deviation)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_arr1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_test3' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
        "print(X_train1.shape, y_test_arr1.shape, np.mean(X_test1), np.std(X_test3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74ZAU27qaI9L",
        "outputId": "fcceeb06-9967-42ed-e911-a774ac4ff47f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5]\n"
          ]
        }
      ],
      "source": [
        "# Label preprocessing with StringLookup.\n",
        "label_processor = tf.keras.layers.IntegerLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(y_train_arr1), mask_token=None\n",
        ")\n",
        "print(label_processor.get_vocabulary())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-attention layers that form the basic blocks of a Transformer are order-agnostic. Since videos are ordered sequences of frames, we need our Transformer model to take into account order information. We do this via positional encoding. We simply embed the positions of the frames present inside videos with an Embedding layer."
      ],
      "metadata": {
        "id": "bNpLNib0ZiXR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JB6bchRWaI9M"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return inputs + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
        "        return mask\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            'position_embeddings': self.position_embeddings,\n",
        "            'sequence_length': self.sequence_length,\n",
        "            'output_dim': self.output_dim\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder layer of Transformer model containing Attention layer, Feed forward layer and normalization layer"
      ],
      "metadata": {
        "id": "IfnTQXJ1Z4F2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp2YUvP5aI9M"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.5\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=tf.nn.gelu), BatchNormalization(), layers.Dropout(0.5),\n",
        "             layers.Dense(embed_dim, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)), BatchNormalization(), layers.Dropout(0.5),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.dropout = layers.Dropout(0.5)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "        inputs = self.layernorm_3(inputs)\n",
        "\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        # attention_output = self.dropout(0.5)(attention_output)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "    def get_config(self):\n",
        "        config = super(EncoderLayer, self).get_config()\n",
        "        config.update({\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'dense_dim': self.dense_dim,\n",
        "            'num_heads': self.num_heads,\n",
        "            'attention': self.attention,\n",
        "            'dense_proj': self.dense_proj,\n",
        "            'layernorm_1': self.layernorm_1,\n",
        "            'layernorm_2': self.layernorm_2,\n",
        "\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder block of Transformer model"
      ],
      "metadata": {
        "id": "v3lHWooLaCus"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoOtc3M9aI9M"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, embed_dim,dense_dim, num_heads, rate=0.1,**kwargs ):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")\n",
        "                           for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)\n",
        "    def get_config(self):\n",
        "        config = super(Encoder, self).get_config()\n",
        "        config.update({\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'num_layers': self.num_layers,\n",
        "            'enc_layers': self.enc_layers,\n",
        "            'dropout': self.dropout,\n",
        "            'dense_dim': self.dense_dim,\n",
        "            'num_heads': self.num_heads\n",
        "        })\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility functions for training\n"
      ],
      "metadata": {
        "id": "bTvc3CMOZ0Km"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEwdi8eDaI9N"
      },
      "outputs": [],
      "source": [
        "def get_compiled_model():\n",
        "    sequence_length = MAX_SEQ_LENGTH\n",
        "    num_layers = 1\n",
        "    embed_dim = NUM_FEATURES\n",
        "    dense_dim = 2\n",
        "    num_heads = 8\n",
        "    classes = len(label_processor.get_vocabulary())\n",
        "\n",
        "\n",
        "\n",
        "    inputs = keras.Input(shape=(None, None))\n",
        "    x = PositionalEmbedding(\n",
        "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
        "    )(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = Encoder(num_layers, embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "      initial_learning_rate=0.00035, decay_steps=10000, decay_rate=0.9)\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    model.compile(\n",
        "\n",
        "\n",
        "        optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFJIDJ-waI9N"
      },
      "outputs": [],
      "source": [
        "def run_experiment(X_train, y_train_arr, X_test, y_test_arr):\n",
        "    filepath = \"Assanali/tmp/video_classifier/\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, monitor='val_accuracy', save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    model = get_compiled_model()\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        y_train_arr,\n",
        "        batch_size=64,\n",
        "        #shuffle = True,\n",
        "        validation_data=(X_test,\n",
        "                        y_test_arr),\n",
        "        # validation_split=0.15,\n",
        "        epochs=150,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    acc = history.history['accuracy']\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "    plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    print('filepath after', os.listdir(filepath))\n",
        "    print('filepath after', model.load_weights(filepath))\n",
        "    model.load_weights(filepath)\n",
        "    # _, accuracy_train = model.evaluate(X_train, y_train_arr)\n",
        "    _, accuracy = model.evaluate(X_test, y_test_arr)\n",
        "    time_start = time.time()\n",
        "    predictions =  model.predict(X_test)\n",
        "    time_stop = time.time()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "    print(\"TOTAL TIME:  {}\".format(time_stop - time_start))\n",
        "\n",
        "    return model, predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjtoDAjlaI9N",
        "outputId": "39852c83-db2a-4c39-d74e-e40571ef6f86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 9.6230 - accuracy: 0.1562 \n",
            "Epoch 00001: val_accuracy improved from -inf to 0.25463, saving model to Assanali/tmp/video_classifier/\n",
            "6/6 [==============================] - 3s 113ms/step - loss: 9.0771 - accuracy: 0.1775 - val_loss: 2.5920 - val_accuracy: 0.2546\n",
            "Epoch 2/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 8.9511 - accuracy: 0.1328\n",
            "Epoch 00002: val_accuracy improved from 0.25463 to 0.43056, saving model to Assanali/tmp/video_classifier/\n",
            "6/6 [==============================] - 0s 48ms/step - loss: 8.2369 - accuracy: 0.1619 - val_loss: 1.6968 - val_accuracy: 0.4306\n",
            "Epoch 3/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 6.1062 - accuracy: 0.2594\n",
            "Epoch 00003: val_accuracy improved from 0.43056 to 0.62963, saving model to Assanali/tmp/video_classifier/\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 6.0145 - accuracy: 0.2611 - val_loss: 1.1557 - val_accuracy: 0.6296\n",
            "Epoch 4/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 5.4144 - accuracy: 0.2734\n",
            "Epoch 00004: val_accuracy improved from 0.62963 to 0.72685, saving model to Assanali/tmp/video_classifier/\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 4.9608 - accuracy: 0.2872 - val_loss: 0.8464 - val_accuracy: 0.7269\n",
            "Epoch 5/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 4.5944 - accuracy: 0.3750\n",
            "Epoch 00005: val_accuracy improved from 0.72685 to 0.76389, saving model to Assanali/tmp/video_classifier/\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 4.4281 - accuracy: 0.3969 - val_loss: 0.6902 - val_accuracy: 0.7639\n",
            "Epoch 6/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 3.0401 - accuracy: 0.4453\n",
            "Epoch 00006: val_accuracy improved from 0.76389 to 0.78704, saving model to Assanali/tmp/video_classifier/\n",
            "6/6 [==============================] - 0s 46ms/step - loss: 3.1839 - accuracy: 0.4752 - val_loss: 0.6063 - val_accuracy: 0.7870\n",
            "Epoch 7/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 3.4029 - accuracy: 0.4766\n",
            "Epoch 00007: val_accuracy improved from 0.78704 to 0.81481, saving model to Assanali/tmp/video_classifier/\n",
            "6/6 [==============================] - 0s 44ms/step - loss: 3.3128 - accuracy: 0.4726 - val_loss: 0.5405 - val_accuracy: 0.8148\n",
            "Epoch 8/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 3.1145 - accuracy: 0.5078\n",
            "Epoch 00008: val_accuracy improved from 0.81481 to 0.83333, saving model to Assanali/tmp/video_classifier/\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 2.9006 - accuracy: 0.5535 - val_loss: 0.4855 - val_accuracy: 0.8333\n",
            "Epoch 9/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 2.6854 - accuracy: 0.5469\n",
            "Epoch 00009: val_accuracy improved from 0.83333 to 0.86111, saving model to Assanali/tmp/video_classifier/\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 2.6615 - accuracy: 0.5640 - val_loss: 0.4438 - val_accuracy: 0.8611\n",
            "Epoch 10/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 2.0777 - accuracy: 0.6406\n",
            "Epoch 00010: val_accuracy improved from 0.86111 to 0.86574, saving model to Assanali/tmp/video_classifier/\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 2.0743 - accuracy: 0.6319 - val_loss: 0.4124 - val_accuracy: 0.8657\n",
            "Epoch 11/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 1.9803 - accuracy: 0.6812\n",
            "Epoch 00011: val_accuracy improved from 0.86574 to 0.88426, saving model to Assanali/tmp/video_classifier/\n",
            "6/6 [==============================] - 0s 45ms/step - loss: 1.9757 - accuracy: 0.6658 - val_loss: 0.3939 - val_accuracy: 0.8843\n",
            "Epoch 12/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 1.7125 - accuracy: 0.6914\n",
            "Epoch 00012: val_accuracy improved from 0.88426 to 0.88889, saving model to Assanali/tmp/video_classifier/\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 1.5154 - accuracy: 0.7102 - val_loss: 0.3912 - val_accuracy: 0.8889\n",
            "Epoch 13/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 1.5862 - accuracy: 0.6836\n",
            "Epoch 00013: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 1.4117 - accuracy: 0.7102 - val_loss: 0.3923 - val_accuracy: 0.8843\n",
            "Epoch 14/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 1.7372 - accuracy: 0.6797\n",
            "Epoch 00014: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 1.5680 - accuracy: 0.7023 - val_loss: 0.3982 - val_accuracy: 0.8750\n",
            "Epoch 15/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 1.2806 - accuracy: 0.7617\n",
            "Epoch 00015: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 1.2164 - accuracy: 0.7650 - val_loss: 0.4151 - val_accuracy: 0.8704\n",
            "Epoch 16/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 1.0724 - accuracy: 0.8164\n",
            "Epoch 00016: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 1.0439 - accuracy: 0.7937 - val_loss: 0.4258 - val_accuracy: 0.8657\n",
            "Epoch 17/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 1.0521 - accuracy: 0.7969\n",
            "Epoch 00017: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 1.0470 - accuracy: 0.7963 - val_loss: 0.4203 - val_accuracy: 0.8657\n",
            "Epoch 18/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 1.0747 - accuracy: 0.7578\n",
            "Epoch 00018: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 1.1858 - accuracy: 0.7363 - val_loss: 0.4278 - val_accuracy: 0.8611\n",
            "Epoch 19/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.8735 - accuracy: 0.7812\n",
            "Epoch 00019: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.8100 - accuracy: 0.7963 - val_loss: 0.4220 - val_accuracy: 0.8657\n",
            "Epoch 20/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 1.0461 - accuracy: 0.7812\n",
            "Epoch 00020: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 1.0641 - accuracy: 0.7885 - val_loss: 0.4224 - val_accuracy: 0.8565\n",
            "Epoch 21/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 1.0017 - accuracy: 0.8281\n",
            "Epoch 00021: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.9424 - accuracy: 0.8198 - val_loss: 0.4124 - val_accuracy: 0.8611\n",
            "Epoch 22/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.7935 - accuracy: 0.8516\n",
            "Epoch 00022: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.8529 - accuracy: 0.8433 - val_loss: 0.4154 - val_accuracy: 0.8796\n",
            "Epoch 23/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.7492 - accuracy: 0.8125\n",
            "Epoch 00023: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.8797 - accuracy: 0.8016 - val_loss: 0.4139 - val_accuracy: 0.8843\n",
            "Epoch 24/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 1.0256 - accuracy: 0.8008\n",
            "Epoch 00024: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.9059 - accuracy: 0.8094 - val_loss: 0.4123 - val_accuracy: 0.8843\n",
            "Epoch 25/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.7925 - accuracy: 0.8359\n",
            "Epoch 00025: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.7008 - accuracy: 0.8512 - val_loss: 0.4168 - val_accuracy: 0.8843\n",
            "Epoch 26/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.7374 - accuracy: 0.8086\n",
            "Epoch 00026: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.7844 - accuracy: 0.8016 - val_loss: 0.4299 - val_accuracy: 0.8796\n",
            "Epoch 27/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.6297 - accuracy: 0.8555\n",
            "Epoch 00027: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 42ms/step - loss: 0.6367 - accuracy: 0.8564 - val_loss: 0.4444 - val_accuracy: 0.8657\n",
            "Epoch 28/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.7296 - accuracy: 0.8281\n",
            "Epoch 00028: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.5912 - accuracy: 0.8616 - val_loss: 0.4544 - val_accuracy: 0.8750\n",
            "Epoch 29/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.7441 - accuracy: 0.8242\n",
            "Epoch 00029: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.7735 - accuracy: 0.8303 - val_loss: 0.4528 - val_accuracy: 0.8796\n",
            "Epoch 30/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.5905 - accuracy: 0.8477\n",
            "Epoch 00030: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.5698 - accuracy: 0.8564 - val_loss: 0.4471 - val_accuracy: 0.8843\n",
            "Epoch 31/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.5915 - accuracy: 0.8438\n",
            "Epoch 00031: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.5834 - accuracy: 0.8564 - val_loss: 0.4576 - val_accuracy: 0.8796\n",
            "Epoch 32/150\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.5781 - accuracy: 0.8668\n",
            "Epoch 00032: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 41ms/step - loss: 0.5781 - accuracy: 0.8668 - val_loss: 0.4634 - val_accuracy: 0.8796\n",
            "Epoch 33/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.5506 - accuracy: 0.8750\n",
            "Epoch 00033: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.5319 - accuracy: 0.8825 - val_loss: 0.4695 - val_accuracy: 0.8796\n",
            "Epoch 34/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.4249 - accuracy: 0.8984\n",
            "Epoch 00034: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.5118 - accuracy: 0.8642 - val_loss: 0.4785 - val_accuracy: 0.8796\n",
            "Epoch 35/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.5046 - accuracy: 0.8672\n",
            "Epoch 00035: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.5339 - accuracy: 0.8773 - val_loss: 0.4842 - val_accuracy: 0.8796\n",
            "Epoch 36/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.6090 - accuracy: 0.8789\n",
            "Epoch 00036: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.5713 - accuracy: 0.8721 - val_loss: 0.4864 - val_accuracy: 0.8750\n",
            "Epoch 37/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.5441 - accuracy: 0.8711\n",
            "Epoch 00037: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.4928 - accuracy: 0.8799 - val_loss: 0.4959 - val_accuracy: 0.8750\n",
            "Epoch 38/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.4524 - accuracy: 0.8906\n",
            "Epoch 00038: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.4419 - accuracy: 0.8851 - val_loss: 0.4928 - val_accuracy: 0.8750\n",
            "Epoch 39/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.4159 - accuracy: 0.9023\n",
            "Epoch 00039: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.4443 - accuracy: 0.8903 - val_loss: 0.4905 - val_accuracy: 0.8796\n",
            "Epoch 40/150\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.4747 - accuracy: 0.9008\n",
            "Epoch 00040: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.4747 - accuracy: 0.9008 - val_loss: 0.4882 - val_accuracy: 0.8750\n",
            "Epoch 41/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.4423 - accuracy: 0.9141\n",
            "Epoch 00041: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.4829 - accuracy: 0.9060 - val_loss: 0.4894 - val_accuracy: 0.8750\n",
            "Epoch 42/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.5898 - accuracy: 0.8555\n",
            "Epoch 00042: val_accuracy did not improve from 0.88889\n",
            "6/6 [==============================] - 0s 29ms/step - loss: 0.5288 - accuracy: 0.8590 - val_loss: 0.4781 - val_accuracy: 0.8750\n",
            "Epoch 43/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.4241 - accuracy: 0.8781\n",
            "Epoch 00043: val_accuracy improved from 0.88889 to 0.90278, saving model to Assanali/tmp/video_classifier/\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.4280 - accuracy: 0.8799 - val_loss: 0.4602 - val_accuracy: 0.9028\n",
            "Epoch 44/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.4178 - accuracy: 0.9000\n",
            "Epoch 00044: val_accuracy did not improve from 0.90278\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.4294 - accuracy: 0.8930 - val_loss: 0.4640 - val_accuracy: 0.9028\n",
            "Epoch 45/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.4636 - accuracy: 0.8969\n",
            "Epoch 00045: val_accuracy did not improve from 0.90278\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.4449 - accuracy: 0.8982 - val_loss: 0.4923 - val_accuracy: 0.8843\n",
            "Epoch 46/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.3588 - accuracy: 0.8969\n",
            "Epoch 00046: val_accuracy did not improve from 0.90278\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 0.3330 - accuracy: 0.9034 - val_loss: 0.5124 - val_accuracy: 0.8843\n",
            "Epoch 47/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.4028 - accuracy: 0.9000\n",
            "Epoch 00047: val_accuracy did not improve from 0.90278\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.4109 - accuracy: 0.8956 - val_loss: 0.5093 - val_accuracy: 0.8843\n",
            "Epoch 48/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.3232 - accuracy: 0.9102\n",
            "Epoch 00048: val_accuracy did not improve from 0.90278\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.3051 - accuracy: 0.9086 - val_loss: 0.4939 - val_accuracy: 0.8935\n",
            "Epoch 49/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.3414 - accuracy: 0.9031\n",
            "Epoch 00049: val_accuracy did not improve from 0.90278\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 0.4049 - accuracy: 0.9008 - val_loss: 0.4865 - val_accuracy: 0.8935\n",
            "Epoch 50/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.3003 - accuracy: 0.9281\n",
            "Epoch 00050: val_accuracy did not improve from 0.90278\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.3125 - accuracy: 0.9217 - val_loss: 0.4963 - val_accuracy: 0.8843\n",
            "Epoch 51/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2907 - accuracy: 0.9281\n",
            "Epoch 00051: val_accuracy did not improve from 0.90278\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.3292 - accuracy: 0.9243 - val_loss: 0.4938 - val_accuracy: 0.8796\n",
            "Epoch 52/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.4166 - accuracy: 0.8875\n",
            "Epoch 00052: val_accuracy did not improve from 0.90278\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.4059 - accuracy: 0.8956 - val_loss: 0.4937 - val_accuracy: 0.8843\n",
            "Epoch 53/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.4542 - accuracy: 0.8813\n",
            "Epoch 00053: val_accuracy did not improve from 0.90278\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.4356 - accuracy: 0.8877 - val_loss: 0.4836 - val_accuracy: 0.8935\n",
            "Epoch 54/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.3452 - accuracy: 0.9094\n",
            "Epoch 00054: val_accuracy did not improve from 0.90278\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.3439 - accuracy: 0.9112 - val_loss: 0.4755 - val_accuracy: 0.8981\n",
            "Epoch 55/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.3081 - accuracy: 0.9250\n",
            "Epoch 00055: val_accuracy did not improve from 0.90278\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.2802 - accuracy: 0.9269 - val_loss: 0.4721 - val_accuracy: 0.8981\n",
            "Epoch 56/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2714 - accuracy: 0.9156\n",
            "Epoch 00056: val_accuracy did not improve from 0.90278\n",
            "6/6 [==============================] - 0s 30ms/step - loss: 0.2763 - accuracy: 0.9138 - val_loss: 0.4746 - val_accuracy: 0.9028\n",
            "Epoch 57/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2584 - accuracy: 0.9344\n",
            "Epoch 00057: val_accuracy improved from 0.90278 to 0.90741, saving model to Assanali/tmp/video_classifier/\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.2637 - accuracy: 0.9295 - val_loss: 0.4762 - val_accuracy: 0.9074\n",
            "Epoch 58/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.3612 - accuracy: 0.9062\n",
            "Epoch 00058: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 30ms/step - loss: 0.3844 - accuracy: 0.9060 - val_loss: 0.4861 - val_accuracy: 0.9074\n",
            "Epoch 59/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.3234 - accuracy: 0.9250\n",
            "Epoch 00059: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 26ms/step - loss: 0.3247 - accuracy: 0.9191 - val_loss: 0.4924 - val_accuracy: 0.9028\n",
            "Epoch 60/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.3209 - accuracy: 0.8969\n",
            "Epoch 00060: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.3013 - accuracy: 0.9060 - val_loss: 0.4958 - val_accuracy: 0.8981\n",
            "Epoch 61/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2847 - accuracy: 0.9250\n",
            "Epoch 00061: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.2703 - accuracy: 0.9243 - val_loss: 0.4953 - val_accuracy: 0.8889\n",
            "Epoch 62/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2280 - accuracy: 0.9187\n",
            "Epoch 00062: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.2457 - accuracy: 0.9191 - val_loss: 0.4912 - val_accuracy: 0.8981\n",
            "Epoch 63/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2864 - accuracy: 0.9125\n",
            "Epoch 00063: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.2713 - accuracy: 0.9164 - val_loss: 0.4907 - val_accuracy: 0.9028\n",
            "Epoch 64/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.3364 - accuracy: 0.9031\n",
            "Epoch 00064: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.3083 - accuracy: 0.9060 - val_loss: 0.4925 - val_accuracy: 0.8935\n",
            "Epoch 65/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2721 - accuracy: 0.9156\n",
            "Epoch 00065: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.2893 - accuracy: 0.9138 - val_loss: 0.4989 - val_accuracy: 0.8889\n",
            "Epoch 66/150\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3284 - accuracy: 0.9295\n",
            "Epoch 00066: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 30ms/step - loss: 0.3284 - accuracy: 0.9295 - val_loss: 0.5086 - val_accuracy: 0.8750\n",
            "Epoch 67/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.3123 - accuracy: 0.9125\n",
            "Epoch 00067: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.2706 - accuracy: 0.9269 - val_loss: 0.5160 - val_accuracy: 0.8796\n",
            "Epoch 68/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2746 - accuracy: 0.9344\n",
            "Epoch 00068: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 30ms/step - loss: 0.2976 - accuracy: 0.9347 - val_loss: 0.5271 - val_accuracy: 0.8796\n",
            "Epoch 69/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2686 - accuracy: 0.9250\n",
            "Epoch 00069: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.2597 - accuracy: 0.9269 - val_loss: 0.5342 - val_accuracy: 0.8843\n",
            "Epoch 70/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.3226 - accuracy: 0.9156\n",
            "Epoch 00070: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 29ms/step - loss: 0.2853 - accuracy: 0.9269 - val_loss: 0.5392 - val_accuracy: 0.8889\n",
            "Epoch 71/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2529 - accuracy: 0.9438\n",
            "Epoch 00071: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 29ms/step - loss: 0.2436 - accuracy: 0.9478 - val_loss: 0.5376 - val_accuracy: 0.8843\n",
            "Epoch 72/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.3158 - accuracy: 0.9281\n",
            "Epoch 00072: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.3021 - accuracy: 0.9321 - val_loss: 0.5281 - val_accuracy: 0.8889\n",
            "Epoch 73/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2593 - accuracy: 0.9156\n",
            "Epoch 00073: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.2530 - accuracy: 0.9243 - val_loss: 0.5293 - val_accuracy: 0.8889\n",
            "Epoch 74/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2536 - accuracy: 0.9312\n",
            "Epoch 00074: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.2676 - accuracy: 0.9295 - val_loss: 0.5415 - val_accuracy: 0.8889\n",
            "Epoch 75/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2403 - accuracy: 0.9312\n",
            "Epoch 00075: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.2433 - accuracy: 0.9347 - val_loss: 0.5445 - val_accuracy: 0.8843\n",
            "Epoch 76/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2681 - accuracy: 0.9344\n",
            "Epoch 00076: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.2733 - accuracy: 0.9269 - val_loss: 0.5391 - val_accuracy: 0.8889\n",
            "Epoch 77/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2290 - accuracy: 0.9375\n",
            "Epoch 00077: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.2327 - accuracy: 0.9373 - val_loss: 0.5342 - val_accuracy: 0.8935\n",
            "Epoch 78/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2527 - accuracy: 0.9281\n",
            "Epoch 00078: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.2309 - accuracy: 0.9321 - val_loss: 0.5148 - val_accuracy: 0.8981\n",
            "Epoch 79/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2680 - accuracy: 0.9281\n",
            "Epoch 00079: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.2863 - accuracy: 0.9269 - val_loss: 0.5088 - val_accuracy: 0.8981\n",
            "Epoch 80/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2441 - accuracy: 0.9312\n",
            "Epoch 00080: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.2214 - accuracy: 0.9373 - val_loss: 0.5147 - val_accuracy: 0.8981\n",
            "Epoch 81/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2358 - accuracy: 0.9344\n",
            "Epoch 00081: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.2344 - accuracy: 0.9399 - val_loss: 0.5210 - val_accuracy: 0.8981\n",
            "Epoch 82/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2234 - accuracy: 0.9469\n",
            "Epoch 00082: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.2438 - accuracy: 0.9399 - val_loss: 0.5191 - val_accuracy: 0.8981\n",
            "Epoch 83/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.1785 - accuracy: 0.9500\n",
            "Epoch 00083: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 30ms/step - loss: 0.1917 - accuracy: 0.9478 - val_loss: 0.5216 - val_accuracy: 0.8981\n",
            "Epoch 84/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2190 - accuracy: 0.9375\n",
            "Epoch 00084: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.2124 - accuracy: 0.9399 - val_loss: 0.5212 - val_accuracy: 0.8981\n",
            "Epoch 85/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.1829 - accuracy: 0.9438\n",
            "Epoch 00085: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 30ms/step - loss: 0.1651 - accuracy: 0.9504 - val_loss: 0.5286 - val_accuracy: 0.8981\n",
            "Epoch 86/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.1988 - accuracy: 0.9500\n",
            "Epoch 00086: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.2000 - accuracy: 0.9452 - val_loss: 0.5326 - val_accuracy: 0.8981\n",
            "Epoch 87/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.2264 - accuracy: 0.9180\n",
            "Epoch 00087: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 26ms/step - loss: 0.2328 - accuracy: 0.9164 - val_loss: 0.5253 - val_accuracy: 0.9028\n",
            "Epoch 88/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2301 - accuracy: 0.9406\n",
            "Epoch 00088: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.2398 - accuracy: 0.9347 - val_loss: 0.5202 - val_accuracy: 0.9074\n",
            "Epoch 89/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.1597 - accuracy: 0.9531\n",
            "Epoch 00089: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.1429 - accuracy: 0.9608 - val_loss: 0.5254 - val_accuracy: 0.9074\n",
            "Epoch 90/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2260 - accuracy: 0.9375\n",
            "Epoch 00090: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.2137 - accuracy: 0.9399 - val_loss: 0.5368 - val_accuracy: 0.9028\n",
            "Epoch 91/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2567 - accuracy: 0.9219\n",
            "Epoch 00091: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.2551 - accuracy: 0.9269 - val_loss: 0.5440 - val_accuracy: 0.9028\n",
            "Epoch 92/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.1620 - accuracy: 0.9594\n",
            "Epoch 00092: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 0.1872 - accuracy: 0.9504 - val_loss: 0.5442 - val_accuracy: 0.9028\n",
            "Epoch 93/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.1910 - accuracy: 0.9563\n",
            "Epoch 00093: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.2208 - accuracy: 0.9478 - val_loss: 0.5518 - val_accuracy: 0.8935\n",
            "Epoch 94/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.1922 - accuracy: 0.9492\n",
            "Epoch 00094: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 27ms/step - loss: 0.1831 - accuracy: 0.9504 - val_loss: 0.5725 - val_accuracy: 0.8889\n",
            "Epoch 95/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2420 - accuracy: 0.9312\n",
            "Epoch 00095: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 0.2286 - accuracy: 0.9373 - val_loss: 0.5940 - val_accuracy: 0.8843\n",
            "Epoch 96/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.2025 - accuracy: 0.9375\n",
            "Epoch 00096: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.2085 - accuracy: 0.9347 - val_loss: 0.5887 - val_accuracy: 0.8843\n",
            "Epoch 97/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.1542 - accuracy: 0.9656\n",
            "Epoch 00097: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 30ms/step - loss: 0.1748 - accuracy: 0.9582 - val_loss: 0.5893 - val_accuracy: 0.8796\n",
            "Epoch 98/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2334 - accuracy: 0.9312\n",
            "Epoch 00098: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 0.2062 - accuracy: 0.9399 - val_loss: 0.5873 - val_accuracy: 0.8796\n",
            "Epoch 99/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2024 - accuracy: 0.9563\n",
            "Epoch 00099: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.1776 - accuracy: 0.9634 - val_loss: 0.5756 - val_accuracy: 0.8796\n",
            "Epoch 100/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2102 - accuracy: 0.9500\n",
            "Epoch 00100: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.2006 - accuracy: 0.9530 - val_loss: 0.5733 - val_accuracy: 0.8796\n",
            "Epoch 101/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.1882 - accuracy: 0.9469\n",
            "Epoch 00101: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.2035 - accuracy: 0.9452 - val_loss: 0.5729 - val_accuracy: 0.8843\n",
            "Epoch 102/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.1555 - accuracy: 0.9469\n",
            "Epoch 00102: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.1452 - accuracy: 0.9504 - val_loss: 0.5753 - val_accuracy: 0.8843\n",
            "Epoch 103/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.1390 - accuracy: 0.9563\n",
            "Epoch 00103: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.1297 - accuracy: 0.9582 - val_loss: 0.5758 - val_accuracy: 0.8750\n",
            "Epoch 104/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.1493 - accuracy: 0.9563\n",
            "Epoch 00104: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.1637 - accuracy: 0.9582 - val_loss: 0.5679 - val_accuracy: 0.8843\n",
            "Epoch 105/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.1393 - accuracy: 0.9625\n",
            "Epoch 00105: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 30ms/step - loss: 0.1327 - accuracy: 0.9634 - val_loss: 0.5645 - val_accuracy: 0.8889\n",
            "Epoch 106/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.1138 - accuracy: 0.9625\n",
            "Epoch 00106: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 0.1052 - accuracy: 0.9687 - val_loss: 0.5570 - val_accuracy: 0.8843\n",
            "Epoch 107/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2006 - accuracy: 0.9500\n",
            "Epoch 00107: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 0.2044 - accuracy: 0.9478 - val_loss: 0.5456 - val_accuracy: 0.8889\n",
            "Epoch 108/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.2046 - accuracy: 0.9414\n",
            "Epoch 00108: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.1627 - accuracy: 0.9556 - val_loss: 0.5369 - val_accuracy: 0.8889\n",
            "Epoch 109/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.1457 - accuracy: 0.9570\n",
            "Epoch 00109: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.1445 - accuracy: 0.9582 - val_loss: 0.5383 - val_accuracy: 0.8843\n",
            "Epoch 110/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.2065 - accuracy: 0.9336\n",
            "Epoch 00110: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 28ms/step - loss: 0.1862 - accuracy: 0.9426 - val_loss: 0.5418 - val_accuracy: 0.8843\n",
            "Epoch 111/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.1336 - accuracy: 0.9625\n",
            "Epoch 00111: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1219 - accuracy: 0.9661 - val_loss: 0.5431 - val_accuracy: 0.8843\n",
            "Epoch 112/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.1932 - accuracy: 0.9492\n",
            "Epoch 00112: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.1658 - accuracy: 0.9530 - val_loss: 0.5392 - val_accuracy: 0.8843\n",
            "Epoch 113/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.2169 - accuracy: 0.9258\n",
            "Epoch 00113: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 0.1820 - accuracy: 0.9347 - val_loss: 0.5430 - val_accuracy: 0.8843\n",
            "Epoch 114/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.1429 - accuracy: 0.9727\n",
            "Epoch 00114: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 29ms/step - loss: 0.1639 - accuracy: 0.9582 - val_loss: 0.5475 - val_accuracy: 0.8843\n",
            "Epoch 115/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.1344 - accuracy: 0.9625\n",
            "Epoch 00115: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1422 - accuracy: 0.9582 - val_loss: 0.5461 - val_accuracy: 0.8843\n",
            "Epoch 116/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.1050 - accuracy: 0.9648\n",
            "Epoch 00116: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.1232 - accuracy: 0.9608 - val_loss: 0.5418 - val_accuracy: 0.8843\n",
            "Epoch 117/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.1606 - accuracy: 0.9648\n",
            "Epoch 00117: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.1515 - accuracy: 0.9634 - val_loss: 0.5316 - val_accuracy: 0.8889\n",
            "Epoch 118/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.2158 - accuracy: 0.9492\n",
            "Epoch 00118: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.1682 - accuracy: 0.9634 - val_loss: 0.5457 - val_accuracy: 0.8843\n",
            "Epoch 119/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.2036 - accuracy: 0.9453\n",
            "Epoch 00119: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.1689 - accuracy: 0.9556 - val_loss: 0.5574 - val_accuracy: 0.8843\n",
            "Epoch 120/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.2204 - accuracy: 0.9492\n",
            "Epoch 00120: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.1853 - accuracy: 0.9582 - val_loss: 0.5589 - val_accuracy: 0.8843\n",
            "Epoch 121/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.1712 - accuracy: 0.9570\n",
            "Epoch 00121: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1513 - accuracy: 0.9608 - val_loss: 0.5626 - val_accuracy: 0.8889\n",
            "Epoch 122/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.2093 - accuracy: 0.9570\n",
            "Epoch 00122: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 29ms/step - loss: 0.1839 - accuracy: 0.9608 - val_loss: 0.5647 - val_accuracy: 0.8843\n",
            "Epoch 123/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.2321 - accuracy: 0.9375\n",
            "Epoch 00123: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.2033 - accuracy: 0.9399 - val_loss: 0.5598 - val_accuracy: 0.8889\n",
            "Epoch 124/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.1283 - accuracy: 0.9531\n",
            "Epoch 00124: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1413 - accuracy: 0.9582 - val_loss: 0.5646 - val_accuracy: 0.8843\n",
            "Epoch 125/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.1304 - accuracy: 0.9648\n",
            "Epoch 00125: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 0.1653 - accuracy: 0.9504 - val_loss: 0.5598 - val_accuracy: 0.8843\n",
            "Epoch 126/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.1014 - accuracy: 0.9766\n",
            "Epoch 00126: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1094 - accuracy: 0.9739 - val_loss: 0.5570 - val_accuracy: 0.8796\n",
            "Epoch 127/150\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0930 - accuracy: 0.9843\n",
            "Epoch 00127: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 0.0930 - accuracy: 0.9843 - val_loss: 0.5610 - val_accuracy: 0.8796\n",
            "Epoch 128/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.0975 - accuracy: 0.9688\n",
            "Epoch 00128: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0844 - accuracy: 0.9713 - val_loss: 0.5726 - val_accuracy: 0.8796\n",
            "Epoch 129/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.1199 - accuracy: 0.9688\n",
            "Epoch 00129: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.1220 - accuracy: 0.9713 - val_loss: 0.5766 - val_accuracy: 0.8796\n",
            "Epoch 130/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.1311 - accuracy: 0.9531\n",
            "Epoch 00130: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1437 - accuracy: 0.9530 - val_loss: 0.5876 - val_accuracy: 0.8843\n",
            "Epoch 131/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.0896 - accuracy: 0.9727\n",
            "Epoch 00131: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.0951 - accuracy: 0.9713 - val_loss: 0.5901 - val_accuracy: 0.8843\n",
            "Epoch 132/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.0703 - accuracy: 0.9844\n",
            "Epoch 00132: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1140 - accuracy: 0.9687 - val_loss: 0.5932 - val_accuracy: 0.8796\n",
            "Epoch 133/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.1254 - accuracy: 0.9688\n",
            "Epoch 00133: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 0.1333 - accuracy: 0.9661 - val_loss: 0.5921 - val_accuracy: 0.8796\n",
            "Epoch 134/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.0875 - accuracy: 0.9727\n",
            "Epoch 00134: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 0.0975 - accuracy: 0.9687 - val_loss: 0.5946 - val_accuracy: 0.8796\n",
            "Epoch 135/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.1184 - accuracy: 0.9688\n",
            "Epoch 00135: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.1159 - accuracy: 0.9634 - val_loss: 0.6036 - val_accuracy: 0.8796\n",
            "Epoch 136/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.0981 - accuracy: 0.9805\n",
            "Epoch 00136: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0873 - accuracy: 0.9817 - val_loss: 0.6126 - val_accuracy: 0.8750\n",
            "Epoch 137/150\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1148 - accuracy: 0.9634\n",
            "Epoch 00137: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 37ms/step - loss: 0.1148 - accuracy: 0.9634 - val_loss: 0.6080 - val_accuracy: 0.8796\n",
            "Epoch 138/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.0812 - accuracy: 0.9844\n",
            "Epoch 00138: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.0895 - accuracy: 0.9765 - val_loss: 0.6069 - val_accuracy: 0.8796\n",
            "Epoch 139/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.1253 - accuracy: 0.9609\n",
            "Epoch 00139: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 0.1111 - accuracy: 0.9687 - val_loss: 0.6159 - val_accuracy: 0.8843\n",
            "Epoch 140/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.1195 - accuracy: 0.9688\n",
            "Epoch 00140: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 29ms/step - loss: 0.1071 - accuracy: 0.9687 - val_loss: 0.6086 - val_accuracy: 0.8843\n",
            "Epoch 141/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.1018 - accuracy: 0.9648\n",
            "Epoch 00141: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 26ms/step - loss: 0.0802 - accuracy: 0.9765 - val_loss: 0.6017 - val_accuracy: 0.8843\n",
            "Epoch 142/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.0910 - accuracy: 0.9805\n",
            "Epoch 00142: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 0.0950 - accuracy: 0.9817 - val_loss: 0.5994 - val_accuracy: 0.8796\n",
            "Epoch 143/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.0800 - accuracy: 0.9781\n",
            "Epoch 00143: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 0.0808 - accuracy: 0.9791 - val_loss: 0.5995 - val_accuracy: 0.8750\n",
            "Epoch 144/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.1341 - accuracy: 0.9727\n",
            "Epoch 00144: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 26ms/step - loss: 0.1268 - accuracy: 0.9739 - val_loss: 0.5962 - val_accuracy: 0.8750\n",
            "Epoch 145/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.1127 - accuracy: 0.9625\n",
            "Epoch 00145: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.1039 - accuracy: 0.9661 - val_loss: 0.6075 - val_accuracy: 0.8750\n",
            "Epoch 146/150\n",
            "4/6 [===================>..........] - ETA: 0s - loss: 0.0850 - accuracy: 0.9805\n",
            "Epoch 00146: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 26ms/step - loss: 0.0942 - accuracy: 0.9791 - val_loss: 0.6235 - val_accuracy: 0.8796\n",
            "Epoch 147/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.0508 - accuracy: 0.9937\n",
            "Epoch 00147: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 0.0542 - accuracy: 0.9922 - val_loss: 0.6233 - val_accuracy: 0.8796\n",
            "Epoch 148/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.0740 - accuracy: 0.9812\n",
            "Epoch 00148: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 0.0767 - accuracy: 0.9765 - val_loss: 0.6204 - val_accuracy: 0.8796\n",
            "Epoch 149/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.1005 - accuracy: 0.9781\n",
            "Epoch 00149: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 27ms/step - loss: 0.0951 - accuracy: 0.9765 - val_loss: 0.6196 - val_accuracy: 0.8750\n",
            "Epoch 150/150\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.0910 - accuracy: 0.9781\n",
            "Epoch 00150: val_accuracy did not improve from 0.90741\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 0.1291 - accuracy: 0.9687 - val_loss: 0.6330 - val_accuracy: 0.8704\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filepath after ['.data-00000-of-00001', 'checkpoint', '.index']\n",
            "filepath after <tensorflow.python.training.tracking.util.CheckpointLoadStatus object at 0x7f6d9c709f10>\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.4762 - accuracy: 0.9074\n",
            "Test accuracy: 90.74%\n",
            "TOTAL TIME:  0.33810877799987793\n"
          ]
        }
      ],
      "source": [
        "trained_model, predictions= run_experiment(X_train1, y_train_arr1, X_test1, y_test_arr1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model.summary()"
      ],
      "metadata": {
        "id": "uHnBZ-b9UKGt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b1e057e-bf29-473c-861b-88a9c6fa34b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, None, None)]      0         \n",
            "                                                                 \n",
            " positional_embedding_3 (Pos  (None, None, 150)        4200      \n",
            " itionalEmbedding)                                               \n",
            "                                                                 \n",
            " dropout_21 (Dropout)        (None, None, 150)         0         \n",
            "                                                                 \n",
            " encoder_3 (Encoder)         (None, None, 150)         726010    \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, None, 150)         0         \n",
            "                                                                 \n",
            " global_max_pooling1d_3 (Glo  (None, 150)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dropout_27 (Dropout)        (None, 150)               0         \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 150)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 6)                 906       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 731,116\n",
            "Trainable params: 730,812\n",
            "Non-trainable params: 304\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jhq138sLFKNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jx4g_n9JFKQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uHAc0BUmFKTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RZPWR_JfFKV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M965Pn4XFKYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eRYDucB9FKa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gGUCJDmfFKdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tRitcAXlFKf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VpMOWkRpFKiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kzcjPm05FKk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3_UqdtoKFKnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dap4WHXXFKpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dTM3D4azFKr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HE5H6O5eFKx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ImmYnFVKFK0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "123oB83oFK2f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZszNSH23aI8B",
        "Jge09JckaI8U",
        "_Jt7hm31aI8e",
        "JXBsmqUDaI8f",
        "fWtFXCr9aI8l",
        "-w8R_mutaI9B",
        "DUyP74xuvjRt"
      ],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}